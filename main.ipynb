{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリをインポート\n",
    "import os #OSに依存する様々な機能を利用するためのモジュール(ファイルやディレクトリ操作など)\n",
    "import re #正規表現を利用するためのモジュール\n",
    "import csv  #csvファイルを扱うためのモジュール\n",
    "import math #数学的計算のためのモジュール\n",
    "import matplotlib.pyplot as plt #グラフ描画のためのモジュール\n",
    "import numpy as np  #多次元配列計算のためのモジュール\n",
    "import pandas as pd #データフレームを扱うためのモジュール\n",
    "from scipy import signal  #信号処理のためのモジュール\n",
    "from scipy.stats import skew, kurtosis  #歪度と尖度を調べるためのモジュール\n",
    "from sklearn.model_selection import train_test_split  #データをトレーニング用とテスト用に分けるためのモジュール\n",
    "from sklearn import preprocessing #データを正規化するためのモジュール\n",
    "from sklearn.preprocessing import StandardScaler  #データを標準化するためのモジュール\n",
    "from sklearn.preprocessing import LabelEncoder  #カテゴリ変数を数値化するためのモジュール\n",
    "from sklearn.linear_model import LinearRegression #線型回帰\n",
    "from sklearn.svm import SVC #サポートベクターマシン\n",
    "from sklearn.ensemble import RandomForestClassifier #ランダムフォレスト\n",
    "from sklearn.neighbors import KNeighborsClassifier  #k-近傍法\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score #機械学習モデルの性能評価のためのモジュール\n",
    "import xgboost as xgb #XGBoost\n",
    "import lightgbm as lgb  #LightGBM\n",
    "import tensorflow as tf #TensorFlow(Googleが開発したオープンソースの機械学習フレームワーク)\n",
    "from tensorflow import keras  #TensorFlow用のニューラルネットワークライブラリAPI\n",
    "from tensorflow.keras import layers #ニューラルネットワークのレイヤーを定義するためのモジュール\n",
    "import torch  #PyTorch\n",
    "import torch.nn as nn #ニューラルネットワークのためのモジュール\n",
    "import torch.optim as optim #パラメータの最適化を行うためのモジュール\n",
    "from torch.utils.data import DataLoader, Dataset  #データをバッチ単位でロードするためのユーティリティクラスz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定数を定義\n",
    "BINS = 4000  #ヒストグラムのビンの数\n",
    "EPSILON = .00001  #スムージングパラメータ\n",
    "UPPER_LIMIT = 1.1 #静止区間の上限\n",
    "LOWER_LIMIT = 0.9 #静止区間の加減\n",
    "STATIONARY_INTERVALS = 5  #静止区間除去のサンプルの間隔(静止区間が何サンプル連続したら除去するか)\n",
    "TRAIN_SIZE = 0.8  #機械学習のトレーニングデータの割合\n",
    "N_ESTIMATORS = 100  #決定木の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ディレクトリ内のAMWS020のデータセットのファイル名と周波数を取得する関数\n",
    "def get_Hz_and_filename(path: str) -> list[int, str]:\n",
    "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
    "    Hz_and_filename=[]  #ファイル名と周波数を格納するリストを宣言\n",
    "\n",
    "    for file in filename:\n",
    "        Hz = re.search(r'\\d+', file)    #正規表現を用いてファイル名の中で一番最初に出てくる数字(周波数)を取得\n",
    "        if Hz:  #数字の入っていないファイル名があるとエラーを吐くので、このif文でチェックする\n",
    "            Hz_and_filename.append([int(Hz.group(0)), file])    #ファイル名と周波数を格納\n",
    "\n",
    "    return Hz_and_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル名と周波数を分けて出力する関数\n",
    "def divide_Hz_and_filename(Hz_and_filename: list[int, str]) -> tuple[list[int], list[str]]:\n",
    "    Hz = []\n",
    "    filename = []\n",
    "    for row in Hz_and_filename:\n",
    "      Hz.append(row[0])\n",
    "      filename.append(row[1])\n",
    "\n",
    "    return Hz, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速度データのCSVファイルから3軸加速度を取得する関数\n",
    "def get_acceleration(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            AccX.append(float(row[2]))\n",
    "            AccY.append(float(row[3]))\n",
    "            AccZ.append(float(row[4]))\n",
    "\n",
    "    return AccX, AccY, AccZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#静止区間を除去する関数\n",
    "def acc_to_remove_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
    "    #各軸の加速度の平均を求める\n",
    "    AvgAccX = sum(AccX) / len(AccX)\n",
    "    AvgAccY = sum(AccY) / len(AccY)\n",
    "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
    "\n",
    "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
    "\n",
    "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
    "\n",
    "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
    "    i = 0 #ループ変数\n",
    "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
    "    while i < len(ResultantAcc):\n",
    "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
    "            counter += 1    #範囲内ならカウントを増やす\n",
    "            if counter == STATIONARY_INTERVALS: #カウントがSTATIONARY_INTERVALSに達したらその区間を削除\n",
    "                del ResultantAcc[i+1-STATIONARY_INTERVALS:i+1]    #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
    "                counter = 0 #カウンターをリセット\n",
    "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
    "        else:\n",
    "            counter = 0 #カウンターをリセット\n",
    "        i += 1\n",
    "\n",
    "    return ResultantAcc  #静止区間を除去した後のリストを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#連続する2サンプルの差分を取る関数\n",
    "def calculate_differences_of_acceleration(ResultantAcc: list[float]) -> list[float]:\n",
    "    DifferenceAcc = [math.fabs(ResultantAcc[i + 1] * 100000 - ResultantAcc[i] * 100000) for i in range(len(ResultantAcc) - 1)]  #100000倍して誤差を取る\n",
    "    return DifferenceAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLダイバージェンス関数 #引数として与える2つの分布は非負の値の集合でなければならないことに注意\n",
    "def KL_divergence(a: list[float], b: list[float]) -> float:\n",
    "    min_value = min(min(a), min(b)) #a,bの最小値の小さい方\n",
    "    max_value = max(max(a), max(b)) #a,bの最大値の大きい方\n",
    "\n",
    "    #a,bのヒストグラムを作成し、同じ数のビンで区切る\n",
    "    a_hist, _ = np.histogram(a, bins=BINS, range=(min_value, max_value))\n",
    "    b_hist, _ = np.histogram(b, bins=BINS, range=(min_value, max_value))\n",
    "\n",
    "    #正規化する(確率分布に変換する、合計を1にする)ために全合計で割る\n",
    "    a_hist = (a_hist + EPSILON) / a_hist.sum()\n",
    "    b_hist = (b_hist + EPSILON) / b_hist.sum()\n",
    "\n",
    "    #KLダイバージェンスの値を返す\n",
    "    return np.sum([ai * np.log(ai / bi) for ai, bi in zip(a_hist, b_hist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSダイバージェンス関数 #引数として与える2つの分布は非負の値の集合でなければならないことに注意\n",
    "def JS_divergence(a: list[float], b: list[float]) -> float:\n",
    "    min_value = min(min(a), min(b)) #a,bの最小値の小さい方\n",
    "    max_value = max(max(a), max(b)) #a,bの最大値の大きい方\n",
    "\n",
    "    #a,bのヒストグラムを作成し、同じ数のビンで区切る\n",
    "    a_hist, _ = np.histogram(a, bins=BINS, range=(min_value, max_value))\n",
    "    b_hist, _ = np.histogram(b, bins=BINS, range=(min_value, max_value))\n",
    "\n",
    "    #正規化する(確率分布に変換する、合計を1にする)ために全合計で割る\n",
    "    a_hist = (a_hist + EPSILON) / a_hist.sum()\n",
    "    b_hist = (b_hist + EPSILON) / b_hist.sum()\n",
    "\n",
    "    #2つの分布の平均値を求める\n",
    "    mean_hist = (a_hist + b_hist) / 2.0\n",
    "\n",
    "    #平均とそれぞれの分布のKLダイバージェンスを算出\n",
    "    kl_a = np.sum([ai * np.log(ai / bi) for ai, bi in zip(a_hist, mean_hist)])\n",
    "    kl_b = np.sum([ai * np.log(ai / bi) for ai, bi in zip(b_hist, mean_hist)])\n",
    "\n",
    "    #JSダイバージェンスの値を返す\n",
    "    return (kl_a + kl_b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データフレームの各行の中で2番目に小さい値が格納されている場所を調べる関数(最小値は同じ確率分布同士の0.0)\n",
    "def get_index_and_columns_of_second_smallest(df: pd.DataFrame) -> list[str, str]:\n",
    "    index_and_columns_of_second_smallest = []  #データフレームの中で2番目に小さい値が格納されている場所のインデックス名とカラム名を格納する変数\n",
    "    for i in range(len(df)):\n",
    "        sorted_row = df.iloc[i].sort_values()   #.ilocでデータフレームの要素を行、列の番号の添字で指定する    #各行の要素を昇順に並び替える\n",
    "        second_smallest_columns = sorted_row.index[1] #各行の2番目に小さい値が格納されているカラム[1]の名前を取得\n",
    "        #second_smallest_label = df.columns.get_loc(second_smallest_index)\n",
    "        index_and_columns_of_second_smallest.append((df.index[i], second_smallest_columns))    #インデックスとカラムのラベル名の組を二次元配列に追加\n",
    "    return index_and_columns_of_second_smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KL・JSダイバージェンスの推定精度を算出する関数\n",
    "def calculate_accuracy(index_and_columns_of_second_smallest: list[str, str]) -> tuple[float, list[int]]:\n",
    "    counter = 0\n",
    "    error_index_list = []\n",
    "    for i in range(len(index_and_columns_of_second_smallest)):\n",
    "        #インデックスとカラムのラベル名が同じならばカウンターを1増やす\n",
    "        if index_and_columns_of_second_smallest[i][0] == index_and_columns_of_second_smallest[i][1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            error_index_list.append(i)\n",
    "            print(f\"間違ってるやつは{i}番目の{index_and_columns_of_second_smallest[i][0]}と{index_and_columns_of_second_smallest[i][1]}です\")\n",
    "\n",
    "    return (counter / len(index_and_columns_of_second_smallest)) * 100, error_index_list  #精度を100分率で返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が全加速度の差分データの最小値〜最大値）\n",
    "def create_histogram(DifferenceAcc_list: list[float]) -> np.histogram:\n",
    "    min_value = min(map(lambda x:max(x), DifferenceAcc_list))   #入力されたリストの中で最も小さい数\n",
    "    max_value = max(map(lambda x:max(x), DifferenceAcc_list))   #入力されたリストの中で最も大きい数\n",
    "\n",
    "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
    "\n",
    "    for i in range(len(DifferenceAcc_list)):\n",
    "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "    return DifferenceAcc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が各加速度の差分データの最小値〜最大値）\n",
    "def create_histogram2(DifferenceAcc_list: list[float]) -> np.histogram:\n",
    "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
    "    for i in range(len(DifferenceAcc_list)):\n",
    "        min_value = min(DifferenceAcc_list[i])\n",
    "        max_value = max(DifferenceAcc_list[i])\n",
    "        #DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "    return DifferenceAcc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速度データのCSVファイルからヒストグラムを作成する一連の流れを自動化した関数\n",
    "def read_Acc_to_histogram(path: str) -> tuple[np.histogram, np.array]:\n",
    "    Hz_and_filename = get_Hz_and_filename(path)\n",
    "    Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "    Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "    Hz = np.array(Hz)\n",
    "\n",
    "    #使う変数を宣言\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    ResultantAcc = []\n",
    "    DifferenceAcc_list = []\n",
    "\n",
    "    #各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "    for i in filename:\n",
    "        AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "        ResultantAcc = acc_to_remove_stationary_intervals(AccX, AccY, AccZ)\n",
    "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc))\n",
    "\n",
    "    DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)\n",
    "    return DifferenceAcc_hist, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各加速度データをダウンサンプリングする関数\n",
    "def resampling_Acc(originHz: int, newHz: int, AccX: list[float], AccY: list[float], AccZ: list[float], Hz: np.array) -> tuple[list[float], list[float], list[float], np.array]:\n",
    "    i = 0   #カウンター変数\n",
    "\n",
    "    while (Hz[i] == originHz):\n",
    "        originlen = len(AccX[i])    #元々のデータの長さ\n",
    "        sampling_factor = int(originlen * (newHz/originHz)) #ダウンサンプリングした後のデータの長さ\n",
    "        newAccX = signal.resample(AccX[i], sampling_factor)    #データをダウンサンプリング\n",
    "        newAccY = signal.resample(AccY[i], sampling_factor)    #データをダウンサンプリング\n",
    "        newAccZ = signal.resample(AccZ[i], sampling_factor)    #データをダウンサンプリング\n",
    "        AccX.append(newAccX)   #ダウンサンプリングデータを加速度データに追加\n",
    "        AccY.append(newAccY)   #ダウンサンプリングデータを加速度データに追加\n",
    "        AccZ.append(newAccZ)   #ダウンサンプリングデータを加速度データに追加\n",
    "        Hz = np.append(Hz, newHz)   #ダウンサンプリングレートを追加\n",
    "        i += 1\n",
    "\n",
    "    return AccX, AccY, AccZ, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLダイバージェンスとJSダイバージェンス算出の一連の流れを自動化した関数\n",
    "def KL_and_JS(path: str):\n",
    "    Hz_and_filename = get_Hz_and_filename(path)\n",
    "    Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "    Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "    Hz = [str(hz) + \"Hz\" for hz in Hz]  #周波数の値+\"Hz\"のリストを作りデータフレームのラベルに用いる\n",
    "\n",
    "    #使う変数を宣言\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    ResultantAcc = []\n",
    "    DifferenceAcc_list = []\n",
    "    resultKLD = [[0.0 for j in range(len(filename))] for i in range(len(filename))]  # resultKLDの要素を0.0で初期化\n",
    "    resultJSD = [[0.0 for j in range(len(filename))] for i in range(len(filename))]  # resultKLDの要素を0.0で初期化\n",
    "    error_index_list = []\n",
    "\n",
    "    #各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "    for i in filename:\n",
    "        AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "        ResultantAcc = acc_to_remove_stationary_intervals(AccX, AccY, AccZ)\n",
    "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc))\n",
    "\n",
    "    #KLダイバージェンスの値を格納\n",
    "    for i in range(len(filename)):\n",
    "        for j in range(len(filename)):\n",
    "            resultKLD[i][j] = KL_divergence(DifferenceAcc_list[i], DifferenceAcc_list[j])\n",
    "\n",
    "    #JSダイバージェンスの値を格納\n",
    "    for i in range(len(filename)):\n",
    "        for j in range(len(filename)):\n",
    "            resultJSD[i][j] = JS_divergence(DifferenceAcc_list[i], DifferenceAcc_list[j])\n",
    "\n",
    "    #結果を出力\n",
    "    df_KLD = pd.DataFrame(resultKLD, index=Hz, columns=Hz)\n",
    "    display(df_KLD)\n",
    "    accuracyKLD, error_index_list = calculate_accuracy(get_index_and_columns_of_second_smallest(df_KLD))\n",
    "    for i in range(len(error_index_list)):\n",
    "        print(filename[error_index_list[i]])\n",
    "    print(f\"KLダイバージェンスによる推定精度は{accuracyKLD}%です\")\n",
    "\n",
    "    df_JSD = pd.DataFrame(resultJSD, index=Hz, columns=Hz)\n",
    "    display(df_JSD)\n",
    "    accuracyJSD, error_index_list = calculate_accuracy(get_index_and_columns_of_second_smallest(df_JSD))\n",
    "    for i in range(len(error_index_list)):\n",
    "        print(filename[error_index_list[i]])\n",
    "    print(f\"JSダイバージェンスによる推定精度は{accuracyJSD}%です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ランダムフォレストによる機械学習モデル構築と性能評価までを自動化した関数\n",
    "def random_forest(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "\n",
    "    # 学習する\n",
    "    clf = RandomForestClassifier(n_estimators=N_ESTIMATORS, random_state=1234)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-近傍法による機械学習モデル構築と性能評価までを自動化した関数\n",
    "def k_neighbors(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(DifferenceAcc_hist)\n",
    "    newdata = scaler.transform(DifferenceAcc_hist)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(newdata, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "\n",
    "    # 学習する\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoostによる機械学習モデル構築と性能評価までを自動化した関数\n",
    "def xgboost(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    Hz = le.fit_transform(Hz)\n",
    "    # 学習する\n",
    "    x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "    clf = xgb.XGBClassifier(objective='multi:softmax')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"my_walk_data(100Hz15minutes)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一連のやつ\n",
    "Hz_and_filename = get_Hz_and_filename(path)\n",
    "Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "Hz = np.array(Hz)\n",
    "\n",
    "#使う変数を宣言\n",
    "readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
    "AccX, AccY, AccZ = [], [], []\n",
    "ResultantAcc = []\n",
    "DifferenceAcc_list = []\n",
    "\n",
    "#各データセットからデータを読み込み二次元配列に格納\n",
    "for i in filename:\n",
    "    readAccX, readAccY, readAccZ = get_acceleration(path+i)\n",
    "    AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
    "\n",
    "#各加速度データをダウンサンプリング\n",
    "for i in range(9, 1, -1):\n",
    "    AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
    "\n",
    "#静止区間を除去\n",
    "for i in range(len(Hz)):\n",
    "    ResultantAcc.append(acc_to_remove_stationary_intervals(AccX[i], AccY[i], AccZ[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒストグラム作成\n",
    "for i in range(len(ResultantAcc)):\n",
    "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
    "DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "       100, 100, 100,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  80,  80,  80,  80,  80,  80,  80,\n",
       "        80,  80,  80,  80,  80,  80,  80,  80,  80,  70,  70,  70,  70,\n",
       "        70,  70,  70,  70,  70,  70,  70,  70,  70,  70,  70,  70,  60,\n",
       "        60,  60,  60,  60,  60,  60,  60,  60,  60,  60,  60,  60,  60,\n",
       "        60,  60,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
       "        50,  50,  50,  50,  50,  40,  40,  40,  40,  40,  40,  40,  40,\n",
       "        40,  40,  40,  40,  40,  40,  40,  40,  30,  30,  30,  30,  30,\n",
       "        30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  20,  20,\n",
       "        20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,\n",
       "        20])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 4000)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DifferenceAcc_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to x_train\n",
    "scaler.fit(DifferenceAcc_hist)\n",
    "\n",
    "# Use the scaler to transform x_train and x_test\n",
    "DifferenceAcc_hist = scaler.transform(DifferenceAcc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer regression\n",
    "#各種パラメータ\n",
    "NUM_HEADS = 8\n",
    "KEY_DIM = 64\n",
    "BINS = 4000\n",
    "DROPOUT = 0.1\n",
    "N = 1\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (BINS,)\n",
    "output_shape = (1,)\n",
    "\n",
    "#形を定義(このモジュールは行列でないとダメっぽい)\n",
    "inputs_encoder = layers.Input(shape=input_shape)\n",
    "inputs_decoder = layers.Input(shape=output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Reshape((10, 400))(inputs_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = layers.Reshape((1, 1))(inputs_decoder)\n",
    "y =tf.tile(y, [1, 10, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Encoder\n",
    "for i in range(N):\n",
    "      #Multi-Head-Attention層\n",
    "      attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(x, x, x)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      attention = layers.Dropout(rate=DROPOUT)(attention)\n",
    "      #Add & Norm層\n",
    "      attention = layers.LayerNormalization()(x + attention)\n",
    "\n",
    "      #Feed Forward層\n",
    "      ffn = layers.Dense(400 * 4, use_bias=True, activation=\"relu\")(attention)\n",
    "      ffn = layers.Dense(400, use_bias=True)(ffn)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      ffn = layers.Dropout(rate=DROPOUT)(ffn)\n",
    "      #Add & Norm層\n",
    "      x = layers.LayerNormalization()(attention + ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Decoder\n",
    "for i in range(N):\n",
    "      #Multi-Head-Attention層\n",
    "      attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(y, y, y)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      attention = layers.Dropout(rate=DROPOUT)(attention)\n",
    "      #Add & Norm層\n",
    "      y = layers.LayerNormalization()(y + attention)\n",
    "\n",
    "      #Multi-Head-Attention層\n",
    "      attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(x, x, y, use_causal_mask=True)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      attention = layers.Dropout(rate=DROPOUT)(attention)\n",
    "      #Add & Norm層\n",
    "      attention = layers.LayerNormalization()(y + attention)\n",
    "\n",
    "      #Feed Forward層\n",
    "      ffn = layers.Dense(400 * 4, use_bias=True, activation=\"relu\")(attention)\n",
    "      ffn = layers.Dense(400, use_bias=True)(ffn)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      ffn = layers.Dropout(rate=DROPOUT)(ffn)\n",
    "      #Add & Norm層\n",
    "      x = layers.LayerNormalization()(attention + ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 10, 400) dtype=float32 (created by layer 'layer_normalization_260')>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[inputs_encoder, inputs_decoder], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    "    metrics=[keras.metrics.mean_squared_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(x)\n",
    "#x = layers.Dense(256, activation=\"relu\")(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    "    metrics=[keras.metrics.mean_squared_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_51 (InputLayer)          [(None, 4000)]       0           []                               \n",
      "                                                                                                  \n",
      " reshape_43 (Reshape)           (None, 10, 400)      0           ['input_51[0][0]']               \n",
      "                                                                                                  \n",
      " multi_head_attention_127 (Mult  (None, 10, 400)     821136      ['reshape_43[0][0]',             \n",
      " iHeadAttention)                                                  'reshape_43[0][0]',             \n",
      "                                                                  'reshape_43[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_137 (Dropout)          (None, 10, 400)      0           ['multi_head_attention_127[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " input_52 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_116 (TFOp  (None, 10, 400)     0           ['reshape_43[0][0]',             \n",
      " Lambda)                                                          'dropout_137[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_44 (Reshape)           (None, 1, 1)         0           ['input_52[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_239 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_116[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " tf.tile_12 (TFOpLambda)        (None, 10, 400)      0           ['reshape_44[0][0]']             \n",
      "                                                                                                  \n",
      " dense_281 (Dense)              (None, 10, 1600)     641600      ['layer_normalization_239[0][0]']\n",
      "                                                                                                  \n",
      " multi_head_attention_128 (Mult  (None, 10, 400)     821136      ['tf.tile_12[0][0]',             \n",
      " iHeadAttention)                                                  'tf.tile_12[0][0]',             \n",
      "                                                                  'tf.tile_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_282 (Dense)              (None, 10, 400)      640400      ['dense_281[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_139 (Dropout)          (None, 10, 400)      0           ['multi_head_attention_128[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_138 (Dropout)          (None, 10, 400)      0           ['dense_282[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_118 (TFOp  (None, 10, 400)     0           ['tf.tile_12[0][0]',             \n",
      " Lambda)                                                          'dropout_139[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_117 (TFOp  (None, 10, 400)     0           ['layer_normalization_239[0][0]',\n",
      " Lambda)                                                          'dropout_138[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_241 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_118[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_240 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_117[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_129 (Mult  (None, 10, 400)     821136      ['layer_normalization_240[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_240[0][0]',\n",
      "                                                                  'layer_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " dropout_140 (Dropout)          (None, 10, 400)      0           ['multi_head_attention_129[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_119 (TFOp  (None, 10, 400)     0           ['layer_normalization_241[0][0]',\n",
      " Lambda)                                                          'dropout_140[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_242 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_119[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " dense_283 (Dense)              (None, 10, 1600)     641600      ['layer_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " dense_284 (Dense)              (None, 10, 400)      640400      ['dense_283[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_141 (Dropout)          (None, 10, 400)      0           ['dense_284[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_120 (TFOp  (None, 10, 400)     0           ['layer_normalization_242[0][0]',\n",
      " Lambda)                                                          'dropout_141[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_243 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_120[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " flatten_24 (Flatten)           (None, 4000)         0           ['layer_normalization_243[0][0]']\n",
      "                                                                                                  \n",
      " dense_285 (Dense)              (None, 128)          512128      ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_142 (Dropout)          (None, 128)          0           ['dense_285[0][0]']              \n",
      "                                                                                                  \n",
      " dense_286 (Dense)              (None, 64)           8256        ['dropout_142[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_143 (Dropout)          (None, 64)           0           ['dense_286[0][0]']              \n",
      "                                                                                                  \n",
      " dense_287 (Dense)              (None, 1)            65          ['dropout_143[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,551,857\n",
      "Trainable params: 5,551,857\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "#予測値をint型に限定\n",
    "#y_pred = tf.cast(model.predict(x_test), tf.int32)\n",
    "#予測値の範囲を限定\n",
    "#y_pred = tf.clip_by_value(model.predict(x_test), 20, 100)\n",
    "\n",
    "#予測値の型と範囲を限定\n",
    "y_pred = tf.cast(tf.clip_by_value(model.predict(x_test), 20, 100), tf.int32)\n",
    "\n",
    "#平均二乗誤差\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "\n",
    "#二乗平均平方誤差\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "#平均絶対誤差\n",
    "mae = mean_absolute_error(y_pred, y_test)\n",
    "\n",
    "#決定係数\n",
    "r2 = r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.620689655172415"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#旧Transformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#transformer regression\n",
    "# Define the input shape\n",
    "input_shape = (BINS,)\n",
    "\n",
    "# Define the model\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "x = layers.Reshape((100, 40))(inputs)\n",
    "#y = layers.Reshape((100, 40))(inputs)\n",
    "x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "#y = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, y)\n",
    "#x = layers.Flatten()(y)\n",
    "x = layers.Flatten()(x)\n",
    "#x = layers.Dense(256, activation=\"relu\")(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    "    metrics=[keras.metrics.mean_squared_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_51 (InputLayer)          [(None, 4000)]       0           []                               \n",
      "                                                                                                  \n",
      " reshape_43 (Reshape)           (None, 10, 400)      0           ['input_51[0][0]']               \n",
      "                                                                                                  \n",
      " multi_head_attention_127 (Mult  (None, 10, 400)     821136      ['reshape_43[0][0]',             \n",
      " iHeadAttention)                                                  'reshape_43[0][0]',             \n",
      "                                                                  'reshape_43[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_137 (Dropout)          (None, 10, 400)      0           ['multi_head_attention_127[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " input_52 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_116 (TFOp  (None, 10, 400)     0           ['reshape_43[0][0]',             \n",
      " Lambda)                                                          'dropout_137[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_44 (Reshape)           (None, 1, 1)         0           ['input_52[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_239 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_116[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " tf.tile_12 (TFOpLambda)        (None, 10, 400)      0           ['reshape_44[0][0]']             \n",
      "                                                                                                  \n",
      " dense_281 (Dense)              (None, 10, 1600)     641600      ['layer_normalization_239[0][0]']\n",
      "                                                                                                  \n",
      " multi_head_attention_128 (Mult  (None, 10, 400)     821136      ['tf.tile_12[0][0]',             \n",
      " iHeadAttention)                                                  'tf.tile_12[0][0]',             \n",
      "                                                                  'tf.tile_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_282 (Dense)              (None, 10, 400)      640400      ['dense_281[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_139 (Dropout)          (None, 10, 400)      0           ['multi_head_attention_128[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_138 (Dropout)          (None, 10, 400)      0           ['dense_282[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_118 (TFOp  (None, 10, 400)     0           ['tf.tile_12[0][0]',             \n",
      " Lambda)                                                          'dropout_139[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_117 (TFOp  (None, 10, 400)     0           ['layer_normalization_239[0][0]',\n",
      " Lambda)                                                          'dropout_138[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_241 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_118[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_240 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_117[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_129 (Mult  (None, 10, 400)     821136      ['layer_normalization_240[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_240[0][0]',\n",
      "                                                                  'layer_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " dropout_140 (Dropout)          (None, 10, 400)      0           ['multi_head_attention_129[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_119 (TFOp  (None, 10, 400)     0           ['layer_normalization_241[0][0]',\n",
      " Lambda)                                                          'dropout_140[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_242 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_119[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " dense_283 (Dense)              (None, 10, 1600)     641600      ['layer_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " dense_284 (Dense)              (None, 10, 400)      640400      ['dense_283[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_141 (Dropout)          (None, 10, 400)      0           ['dense_284[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_120 (TFOp  (None, 10, 400)     0           ['layer_normalization_242[0][0]',\n",
      " Lambda)                                                          'dropout_141[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_243 (Layer  (None, 10, 400)     800         ['tf.__operators__.add_120[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " flatten_24 (Flatten)           (None, 4000)         0           ['layer_normalization_243[0][0]']\n",
      "                                                                                                  \n",
      " dense_285 (Dense)              (None, 128)          512128      ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_142 (Dropout)          (None, 128)          0           ['dense_285[0][0]']              \n",
      "                                                                                                  \n",
      " dense_286 (Dense)              (None, 64)           8256        ['dropout_142[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_143 (Dropout)          (None, 64)           0           ['dense_286[0][0]']              \n",
      "                                                                                                  \n",
      " dense_287 (Dense)              (None, 1)            65          ['dropout_143[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,551,857\n",
      "Trainable params: 5,551,857\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 100.5328 - mean_squared_error: 100.5328 - val_loss: 49.6650 - val_mean_squared_error: 49.6650\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 100.0576 - mean_squared_error: 100.0576 - val_loss: 61.4408 - val_mean_squared_error: 61.4408\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 79.0702 - mean_squared_error: 79.0702 - val_loss: 42.4341 - val_mean_squared_error: 42.4341\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 82.1085 - mean_squared_error: 82.1085 - val_loss: 66.1347 - val_mean_squared_error: 66.1347\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 60.8872 - mean_squared_error: 60.8872 - val_loss: 43.3535 - val_mean_squared_error: 43.3535\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 92.5453 - mean_squared_error: 92.5453 - val_loss: 44.5976 - val_mean_squared_error: 44.5976\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 60.5816 - mean_squared_error: 60.5816 - val_loss: 103.9696 - val_mean_squared_error: 103.9696\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 88.7827 - mean_squared_error: 88.7827 - val_loss: 25.8037 - val_mean_squared_error: 25.8037\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 44.2077 - mean_squared_error: 44.2077 - val_loss: 69.8748 - val_mean_squared_error: 69.8748\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 61.4619 - mean_squared_error: 61.4619 - val_loss: 31.0495 - val_mean_squared_error: 31.0495\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 57.8671 - mean_squared_error: 57.8671 - val_loss: 42.5087 - val_mean_squared_error: 42.5087\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 47.4309 - mean_squared_error: 47.4309 - val_loss: 46.8663 - val_mean_squared_error: 46.8663\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 61.7386 - mean_squared_error: 61.7386 - val_loss: 23.7007 - val_mean_squared_error: 23.7007\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 52.4179 - mean_squared_error: 52.4179 - val_loss: 33.8584 - val_mean_squared_error: 33.8584\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 66.5153 - mean_squared_error: 66.5153 - val_loss: 47.2484 - val_mean_squared_error: 47.2484\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 62.1053 - mean_squared_error: 62.1053 - val_loss: 25.1163 - val_mean_squared_error: 25.1163\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.8190 - mean_squared_error: 49.8190 - val_loss: 36.0747 - val_mean_squared_error: 36.0747\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 54.7370 - mean_squared_error: 54.7370 - val_loss: 14.5007 - val_mean_squared_error: 14.5007\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 26.8344 - mean_squared_error: 26.8344 - val_loss: 41.5036 - val_mean_squared_error: 41.5036\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 49.5731 - mean_squared_error: 49.5731 - val_loss: 20.1548 - val_mean_squared_error: 20.1548\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 44.5845 - mean_squared_error: 44.5845 - val_loss: 36.9530 - val_mean_squared_error: 36.9530\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 41.7268 - mean_squared_error: 41.7268 - val_loss: 19.9749 - val_mean_squared_error: 19.9749\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 45.9053 - mean_squared_error: 45.9053 - val_loss: 26.8818 - val_mean_squared_error: 26.8818\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 40.2781 - mean_squared_error: 40.2781 - val_loss: 17.5387 - val_mean_squared_error: 17.5387\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.6780 - mean_squared_error: 42.6780 - val_loss: 37.8093 - val_mean_squared_error: 37.8093\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 57.1811 - mean_squared_error: 57.1811 - val_loss: 17.2827 - val_mean_squared_error: 17.2827\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 41.4217 - mean_squared_error: 41.4217 - val_loss: 27.7965 - val_mean_squared_error: 27.7965\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.7811 - mean_squared_error: 43.7811 - val_loss: 19.6646 - val_mean_squared_error: 19.6646\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2743 - mean_squared_error: 32.2743 - val_loss: 26.0484 - val_mean_squared_error: 26.0484\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 35.3219 - mean_squared_error: 35.3219 - val_loss: 18.5118 - val_mean_squared_error: 18.5118\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.4619 - mean_squared_error: 32.4619 - val_loss: 26.0380 - val_mean_squared_error: 26.0380\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 41.0445 - mean_squared_error: 41.0445 - val_loss: 18.9673 - val_mean_squared_error: 18.9673\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4429 - mean_squared_error: 34.4429 - val_loss: 21.0764 - val_mean_squared_error: 21.0764\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 51.4335 - mean_squared_error: 51.4335 - val_loss: 57.5094 - val_mean_squared_error: 57.5094\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 41.2649 - mean_squared_error: 41.2649 - val_loss: 19.8690 - val_mean_squared_error: 19.8690\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 62.1133 - mean_squared_error: 62.1133 - val_loss: 62.7363 - val_mean_squared_error: 62.7363\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 57.1723 - mean_squared_error: 57.1723 - val_loss: 24.0968 - val_mean_squared_error: 24.0968\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 39.8680 - mean_squared_error: 39.8680 - val_loss: 24.5389 - val_mean_squared_error: 24.5389\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 59.3535 - mean_squared_error: 59.3535 - val_loss: 47.4907 - val_mean_squared_error: 47.4907\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 48.8533 - mean_squared_error: 48.8533 - val_loss: 25.5720 - val_mean_squared_error: 25.5720\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 68.2489 - mean_squared_error: 68.2489 - val_loss: 34.7374 - val_mean_squared_error: 34.7374\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.5792 - mean_squared_error: 36.5792 - val_loss: 40.8607 - val_mean_squared_error: 40.8607\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 39.7031 - mean_squared_error: 39.7031 - val_loss: 23.1374 - val_mean_squared_error: 23.1374\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 40.7874 - mean_squared_error: 40.7874 - val_loss: 22.9757 - val_mean_squared_error: 22.9757\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 39.9052 - mean_squared_error: 39.9052 - val_loss: 39.3704 - val_mean_squared_error: 39.3704\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 39.9195 - mean_squared_error: 39.9195 - val_loss: 34.8177 - val_mean_squared_error: 34.8177\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.0015 - mean_squared_error: 42.0015 - val_loss: 45.7850 - val_mean_squared_error: 45.7850\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 54.5435 - mean_squared_error: 54.5435 - val_loss: 14.3818 - val_mean_squared_error: 14.3818\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 40.7715 - mean_squared_error: 40.7715 - val_loss: 12.1364 - val_mean_squared_error: 12.1364\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 41.3057 - mean_squared_error: 41.3057 - val_loss: 34.4858 - val_mean_squared_error: 34.4858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa1118d0>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train, y_train], y_train, batch_size=16, epochs=50, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.cast(tf.clip_by_value(model.predict([x_test, y_test]), 20, 100), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2758620689655173"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(29, 1), dtype=int32, numpy=\n",
       "array([[64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64],\n",
       "       [64]], dtype=int32)>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  60,  60, 100,  50,  30,  80,  80,  60,  60,  80,  60,  50,\n",
       "        90,  80,  60,  40,  90,  60,  90,  50, 100,  90,  90,  30,  70,\n",
       "        70,  20, 100])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一連のやつ\n",
    "Hz_and_filename = get_Hz_and_filename(path)\n",
    "Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "Hz = np.array(Hz)\n",
    "\n",
    "#使う変数を宣言\n",
    "AccX, AccY, AccZ = [], [], []\n",
    "ResultantAcc = []\n",
    "DifferenceAcc_list = []\n",
    "\n",
    "#各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "for i in filename:\n",
    "    AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "    ResultantAcc.append(acc_to_remove_stationary_intervals(AccX, AccY, AccZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ResultantAcc)):\n",
    "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
    "DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#尖度\n",
    "DifferenceAcc_kurtosis = np.zeros(len(DifferenceAcc_hist))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    DifferenceAcc_kurtosis[i] = kurtosis(DifferenceAcc_hist[i])\n",
    "\n",
    "#歪度\n",
    "DifferenceAcc_skewness = np.zeros(len(DifferenceAcc_hist))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    DifferenceAcc_skewness[i] = skew(DifferenceAcc_hist[i])\n",
    "\n",
    "#分散\n",
    "histogram_var = np.zeros(len(DifferenceAcc_list))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    histogram_var[i] = np.var(DifferenceAcc_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_newに尖度＋歪度+分散を連結したもの\n",
    "X_new = np.concatenate((DifferenceAcc_kurtosis.reshape(-1, 1), DifferenceAcc_skewness.reshape(-1, 1), histogram_var.reshape(-1, 1)), axis=1)\n",
    "#XはDifferenceAcc_hist+X_new\n",
    "X = np.concatenate((DifferenceAcc_hist, X_new), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "919e6181955fbc636a96e4fdb04fb1b969c9681582829f05a2534c8d07862e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
