{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oTsgcWLEa9Nt",
        "outputId": "2597428e-c6c6-4973-da09-f7ac58e7d93b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize_matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from japanize_matplotlib) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->japanize_matplotlib) (1.16.0)\n",
            "Building wheels for collected packages: japanize_matplotlib\n",
            "  Building wheel for japanize_matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize_matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=e878d5f7aff21e82c0d38d26e8e323123cc927ec24e5d94919e49007fd938f4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/7a/6b/df1f79be9c59862525070e157e62b08eab8ece27c1b68fbb94\n",
            "Successfully built japanize_matplotlib\n",
            "Installing collected packages: japanize_matplotlib\n",
            "Successfully installed japanize_matplotlib-1.1.3\n"
          ]
        }
      ],
      "source": [
        "pip install japanize_matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q1CJ_gIv04kN",
        "outputId": "6108f2a6-086f-49db-8261-022703b6cc88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/354.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/354.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.0/354.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bottleneck) (1.23.5)\n",
            "Installing collected packages: bottleneck\n",
            "Successfully installed bottleneck-1.3.7\n"
          ]
        }
      ],
      "source": [
        "pip install bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ukwaNImx0hMi"
      },
      "outputs": [],
      "source": [
        "#ライブラリをインポート\n",
        "import os #OSに依存する様々な機能を利用するためのモジュール(ファイルやディレクトリ操作など)\n",
        "import re #正規表現を利用するためのモジュール\n",
        "import csv  #csvファイルを扱うためのモジュール\n",
        "import statistics #数学的計算のためのモジュール\n",
        "import math #数学的計算のためのモジュール\n",
        "from decimal import Decimal #小数点桁落ちをなくすためのモジュール\n",
        "import matplotlib.pyplot as plt #グラフ描画のためのモジュール\n",
        "import japanize_matplotlib #グラフ描画に日本語を用いるためのモジュール\n",
        "import numpy as np  #多次元配列計算のためのモジュール\n",
        "import bottleneck as bn #移動平均計算のためのモジュール\n",
        "import pandas as pd #データフレームを扱うためのモジュール\n",
        "from scipy import signal  #信号処理のためのモジュール\n",
        "from scipy.stats import skew, kurtosis  #歪度と尖度を調べるためのモジュール\n",
        "from sklearn.model_selection import train_test_split  #データをトレーニング用とテスト用に分けるためのモジュール\n",
        "from sklearn import preprocessing #データを正規化するためのモジュール\n",
        "from sklearn.preprocessing import StandardScaler  #データを標準化するためのモジュール\n",
        "from sklearn.preprocessing import LabelEncoder  #カテゴリ変数を数値化するためのモジュール\n",
        "from sklearn.preprocessing import PolynomialFeatures  #多項式回帰のためのモジュール\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score #機械学習モデルの性能評価のためのモジュール\n",
        "import tensorflow as tf #TensorFlow(Googleが開発したオープンソースの機械学習フレームワーク)\n",
        "from tensorflow import keras  #TensorFlow用のニューラルネットワークライブラリAPI\n",
        "from tensorflow.keras import layers #ニューラルネットワークのレイヤーを定義するためのモジュール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CrXcc8k3bDt9",
        "outputId": "02376d80-742a-407d-c9b8-c6b106478d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Google colab用\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H5OsmIbDSvG2",
        "outputId": "e84ffb1d-2997-4a5b-8607-d39e5e25900a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "#GPUを使うためのコマンド\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cuJ52pHW_ajE"
      },
      "outputs": [],
      "source": [
        "#よく使うgit command\n",
        "\n",
        "#google colabと揃える\n",
        "#git fetch origin main\n",
        "#git reset --hard origin/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-E1jV0_ybIY8"
      },
      "outputs": [],
      "source": [
        "#定数を定義\n",
        "BINS = 4000  #ヒストグラムのビンの数\n",
        "EPSILON = .00001  #スムージングパラメータ\n",
        "UPPER_LIMIT = 1.1 #静止区間の上限\n",
        "LOWER_LIMIT = 0.9 #静止区間の加減\n",
        "STATIONARY_INTERVALS = 5  #静止区間除去のサンプルの間隔(静止区間が何サンプル連続したら除去するか)\n",
        "TRAIN_SIZE = 0.8  #機械学習のトレーニングデータの割合\n",
        "\n",
        "#transformer regression\n",
        "#各種パラメータ\n",
        "NUM_HEADS = 8\n",
        "KEY_DIM = 500\n",
        "DROPOUT = 0.1\n",
        "N = 1 #Encoderのレイヤー"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MBcD5SbVbtzz"
      },
      "outputs": [],
      "source": [
        "#AMSW020の加速度データのCSVファイルから3軸加速度を取得する関数\n",
        "def get_acceleration(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    with open(filename) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            AccX.append(float(row[2]))\n",
        "            AccY.append(float(row[3]))\n",
        "            AccZ.append(float(row[4]))\n",
        "\n",
        "    return AccX, AccY, AccZ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#selfBACKとKU_HARの加速度データのCSVファイルから3軸加速度を取得する関数\n",
        "def get_acceleration2(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    with open(filename) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            AccX.append(float(row[1]))\n",
        "            AccY.append(float(row[2]))\n",
        "            AccZ.append(float(row[3]))\n",
        "\n",
        "    return AccX, AccY, AccZ"
      ],
      "metadata": {
        "id": "5_MasOzef-ga"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PAMAP2の加速度データのCSVファイルから3軸加速度を取得する関数\n",
        "def get_acceleration3(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    with open(filename) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            AccX.append(float(row[4]))\n",
        "            AccY.append(float(row[5]))\n",
        "            AccZ.append(float(row[6]))\n",
        "\n",
        "    return AccX, AccY, AccZ"
      ],
      "metadata": {
        "id": "RxvKU_Ddf_4f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NxK06DTFWWfJ"
      },
      "outputs": [],
      "source": [
        "#3軸合成加速度を計算する関数\n",
        "def acc_to_resultant(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
        "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
        "    return ResultantAcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ryj7pQ-TbxHG"
      },
      "outputs": [],
      "source": [
        "#静止区間を除去する関数(STATIONARY_INTERVALS分のみ)\n",
        "def acc_to_remove_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
        "    #各軸の加速度の平均を求める\n",
        "    AvgAccX = sum(AccX) / len(AccX)\n",
        "    AvgAccY = sum(AccY) / len(AccY)\n",
        "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
        "\n",
        "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
        "\n",
        "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
        "\n",
        "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
        "    i = 0 #ループ変数\n",
        "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
        "    while i < len(ResultantAcc):\n",
        "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
        "            counter += 1    #範囲内ならカウントを増やす\n",
        "            if counter == STATIONARY_INTERVALS: #カウントがSTATIONARY_INTERVALSに達したらその区間を削除\n",
        "                del ResultantAcc[i+1-STATIONARY_INTERVALS:i+1]    #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
        "                counter = 0 #カウンターをリセット\n",
        "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
        "        else:\n",
        "            counter = 0 #カウンターをリセット\n",
        "        i += 1\n",
        "\n",
        "    return ResultantAcc  #静止区間を除去した後のリストを返す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8bWHh1zqSvG6"
      },
      "outputs": [],
      "source": [
        "#静止区間を除去する関数(STATIONARY_INTERVALS以上続いた静止区間全て)\n",
        "def acc_to_remove_all_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
        "    #各軸の加速度の平均を求める\n",
        "    AvgAccX = sum(AccX) / len(AccX)\n",
        "    AvgAccY = sum(AccY) / len(AccY)\n",
        "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
        "\n",
        "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
        "\n",
        "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
        "\n",
        "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
        "    i = 0 #ループ変数\n",
        "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
        "    while i < len(ResultantAcc):\n",
        "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
        "            counter += 1    #範囲内ならカウントを増やす\n",
        "        else:\n",
        "            if counter >= STATIONARY_INTERVALS:\n",
        "                del ResultantAcc[i+1-counter:i+1]   #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
        "                counter = 0 #カウンターをリセット\n",
        "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
        "            else :\n",
        "                counter = 0 #カウンターをリセット\n",
        "        i += 1\n",
        "\n",
        "    return ResultantAcc  #静止区間を除去した後のリストを返す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UVjjm5C493em"
      },
      "outputs": [],
      "source": [
        "#連続する2サンプルの差分を取る関数\n",
        "def calculate_differences_of_acceleration(ResultantAcc: list[float]) -> list[float]:\n",
        "    DifferenceAcc = [math.fabs(Decimal(ResultantAcc[i + 1]) - Decimal(ResultantAcc[i])) for i in range(len(ResultantAcc) - 1)]  #連続する2サンプルの誤差を取る\n",
        "    return DifferenceAcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hbUSs34Ib4Z_"
      },
      "outputs": [],
      "source": [
        "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が各加速度の差分データの最小値〜最大値）\n",
        "def create_histogram(DifferenceAcc_list: list[float]) -> np.histogram:\n",
        "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
        "    for i in range(len(DifferenceAcc_list)):\n",
        "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, density=True) #ヒストグラムを作成し、同じ数のビンで区切る\n",
        "    return DifferenceAcc_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZDGYX0wZ6wzf"
      },
      "outputs": [],
      "source": [
        "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が全加速度の差分データの中の最小値〜最大値）\n",
        "def create_histogram2(DifferenceAcc_list: list[float]) -> np.histogram:\n",
        "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
        "    min_value = min(min(row) for row in DifferenceAcc_list)\n",
        "    max_value = max(max(row) for row in DifferenceAcc_list)\n",
        "    for i in range(len(DifferenceAcc_list)):\n",
        "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value), density=True) #ヒストグラムを作成し、同じ数のビンで区切る\n",
        "    return DifferenceAcc_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BqTU8HO0BrDC"
      },
      "outputs": [],
      "source": [
        "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が0~固定値)\n",
        "def create_histogram3(DifferenceAcc_list: list[float]) -> np.histogram:\n",
        "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
        "    for i in range(len(DifferenceAcc_list)):\n",
        "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(0, MAX_VALUE), density=True) #ヒストグラムを作成し、同じ数のビンで区切る\n",
        "    return DifferenceAcc_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DqeTnb2lb9wM"
      },
      "outputs": [],
      "source": [
        "#各加速度データをダウンサンプリングする関数\n",
        "def resampling_Acc(originHz: int, newHz: int, AccX: list[float], AccY: list[float], AccZ: list[float], Hz: np.array) -> tuple[list[float], list[float], list[float], np.array]:\n",
        "    i = 0   #カウンター変数\n",
        "\n",
        "    while (Hz[i] == originHz):\n",
        "        originlen = len(AccX[i])    #元々のデータの長さ\n",
        "        sampling_factor = int(originlen * (newHz/originHz)) #ダウンサンプリングした後のデータの長さ\n",
        "        newAccX = signal.resample(AccX[i], sampling_factor)    #データをダウンサンプリング\n",
        "        newAccY = signal.resample(AccY[i], sampling_factor)    #データをダウンサンプリング\n",
        "        newAccZ = signal.resample(AccZ[i], sampling_factor)    #データをダウンサンプリング\n",
        "        AccX.append(newAccX)   #ダウンサンプリングデータを加速度データに追加\n",
        "        AccY.append(newAccY)   #ダウンサンプリングデータを加速度データに追加\n",
        "        AccZ.append(newAccZ)   #ダウンサンプリングデータを加速度データに追加\n",
        "        Hz = np.append(Hz, newHz)   #ダウンサンプリングレートを追加\n",
        "        i += 1\n",
        "\n",
        "    return AccX, AccY, AccZ, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LsoR8BiPsy32"
      },
      "outputs": [],
      "source": [
        "#My Transformer Model\n",
        "def Transformer_Encoder_B():\n",
        "      # Define the input shape\n",
        "      input_shape = (BINS,)\n",
        "      output_shape = (1,)\n",
        "\n",
        "      #形を定義(このモジュールは行列でないとダメっぽい)\n",
        "      inputs_encoder = layers.Input(shape=input_shape)\n",
        "      #inputs_decoder = layers.Input(shape=output_shape)\n",
        "\n",
        "      #Encoderに対する入力の形状\n",
        "      x_encoder = layers.Reshape((10, 400))(inputs_encoder)\n",
        "\n",
        "      #Transformer Encoder Layer(BERT)\n",
        "      for i in range(N):\n",
        "            #Multi-Head-Attention Layer\n",
        "            attention_encoder = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM, use_bias=True)(x_encoder, x_encoder, x_encoder)\n",
        "\n",
        "            #Dropout Layer\n",
        "            attention_encoder = layers.Dropout(rate=DROPOUT)(attention_encoder)\n",
        "            #Add & Norm Layer\n",
        "            attention_encoder = layers.LayerNormalization()(x_encoder + attention_encoder)\n",
        "\n",
        "            #Feed-Forward-Network\n",
        "            ffn_encoder = layers.Dense(400 * 4, use_bias=True, activation=\"relu\")(attention_encoder)\n",
        "            ffn_encoder = layers.Dense(400, use_bias=True)(ffn_encoder)\n",
        "\n",
        "            #Dropout Layer\n",
        "            ffn_encoder = layers.Dropout(rate=DROPOUT)(ffn_encoder)\n",
        "            #Add & Norm Layer\n",
        "            x_encoder = layers.LayerNormalization()(attention_encoder + ffn_encoder)\n",
        "\n",
        "      x = layers.Flatten()(x_encoder)\n",
        "      x = layers.Dense(128, activation=\"relu\")(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      x = layers.Dense(64, activation=\"relu\")(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      outputs = layers.Dense(1, activation=\"relu\")(x)\n",
        "\n",
        "      model = keras.Model(inputs=inputs_encoder, outputs=outputs)\n",
        "\n",
        "      # Compile the model\n",
        "      model.compile(\n",
        "            optimizer=keras.optimizers.Adam(),\n",
        "            loss=keras.losses.mean_squared_error,\n",
        "            metrics=[keras.metrics.mean_squared_error],\n",
        "      )\n",
        "\n",
        "      return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GtcdpauwpgvR"
      },
      "outputs": [],
      "source": [
        "#My Transformer Model\n",
        "def Transformer_Encoder_S():\n",
        "      # Define the input shape\n",
        "      input_shape = (BINS,)\n",
        "      output_shape = (1,)\n",
        "\n",
        "      #形を定義(このモジュールは行列でないとダメっぽい)\n",
        "      inputs_encoder = layers.Input(shape=input_shape)\n",
        "      #inputs_decoder = layers.Input(shape=output_shape)\n",
        "\n",
        "      #Encoderに対する入力の形状\n",
        "      x_encoder = layers.Reshape((1, 100))(inputs_encoder)\n",
        "\n",
        "      #Transformer Encoder Layer(BERT)\n",
        "      for i in range(N):\n",
        "            #Multi-Head-Attention Layer\n",
        "            attention_encoder = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM, use_bias=True)(x_encoder, x_encoder, x_encoder)\n",
        "\n",
        "            #Dropout Layer\n",
        "            attention_encoder = layers.Dropout(rate=DROPOUT)(attention_encoder)\n",
        "            #Add & Norm Layer\n",
        "            attention_encoder = layers.LayerNormalization()(x_encoder + attention_encoder)\n",
        "\n",
        "            #Feed-Forward-Network\n",
        "            ffn_encoder = layers.Dense(BINS * 4, use_bias=True, activation=\"relu\")(attention_encoder)\n",
        "            ffn_encoder = layers.Dense(BINS, use_bias=True)(ffn_encoder)\n",
        "\n",
        "            #Dropout Layer\n",
        "            ffn_encoder = layers.Dropout(rate=DROPOUT)(ffn_encoder)\n",
        "            #Add & Norm Layer\n",
        "            x_encoder = layers.LayerNormalization()(attention_encoder + ffn_encoder)\n",
        "\n",
        "      x = layers.Flatten()(x_encoder)\n",
        "      x = layers.Dense(32, activation=\"relu\")(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      x = layers.Dense(16, activation=\"relu\")(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      outputs = layers.Dense(1, activation=\"relu\")(x)\n",
        "\n",
        "      model = keras.Model(inputs=inputs_encoder, outputs=outputs)\n",
        "\n",
        "      # Compile the model\n",
        "      model.compile(\n",
        "            optimizer=keras.optimizers.Adam(),\n",
        "            loss=keras.losses.mean_squared_error,\n",
        "            metrics=[keras.metrics.mean_squared_error],\n",
        "      )\n",
        "\n",
        "      return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gp5b0edH5qfd"
      },
      "outputs": [],
      "source": [
        "#path(サンプリング周波数100Hzのデータ)から加速度の差分値, ヒストグラム, 正解ラベルを返す関数\n",
        "def path_to_histogram(path: str) -> tuple[list[float], np.array, np.array]:\n",
        "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "    #使う変数を宣言\n",
        "    readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    ResultantAcc = []\n",
        "    DifferenceAcc_list = []\n",
        "\n",
        "    #各データセットからデータを読み込み二次元配列に格納\n",
        "    for i in filename:\n",
        "        readAccX, readAccY, readAccZ = get_acceleration_self_KUHAR(path+i)\n",
        "        AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "    Hz = np.ones(len(filename)) * 100\n",
        "\n",
        "    #各加速度データをダウンサンプリング\n",
        "    for i in range(9, 1, -1):\n",
        "        AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
        "\n",
        "    #静止区間を除去\n",
        "    for i in range(len(Hz)):\n",
        "        ResultantAcc.append(acc_to_remove_all_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "    #ヒストグラム作成\n",
        "    for i in range(len(ResultantAcc)):\n",
        "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "    DifferenceAcc_hist = create_histogram3(DifferenceAcc_list)\n",
        "\n",
        "    return DifferenceAcc_list, DifferenceAcc_hist, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "cT-Q5MevWWfN"
      },
      "outputs": [],
      "source": [
        "#path(サンプリング周波数100Hzのデータ)から様々な特徴量と正解ラベルを返す関数\n",
        "def path_to_features(path: str) -> tuple[np.array, np.array]:\n",
        "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "    #使う変数を宣言\n",
        "    readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    ResultantAcc = []\n",
        "    DifferenceAcc_list = []\n",
        "\n",
        "    #各データセットからデータを読み込み二次元配列に格納\n",
        "    for i in filename:\n",
        "        #readAccX, readAccY, readAccZ = get_acceleration(path+i)\n",
        "        readAccX, readAccY, readAccZ = get_acceleration2(path+i)\n",
        "        #readAccX, readAccY, readAccZ = get_acceleration3(path+i)\n",
        "        AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "    Hz = np.ones(len(filename)) * 100\n",
        "\n",
        "    #各加速度データをダウンサンプリング\n",
        "    for i in range(9, 1, -1):\n",
        "        AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
        "\n",
        "    features = np.ones((len(Hz), 8))    #各特徴量を入れる変数でこの関数の返り値\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "        ResultantAcc.append(acc_to_resultant(AccX[i], AccY[i], AccZ[i]))\n",
        "        features[i][0] = np.var(ResultantAcc[i])   #静止区間を除去する前の合成加速度の分散値を0列目に格納\n",
        "        features[i][1] = len(ResultantAcc[i])   #静止区間を除去する前のデータの長さを1列目に格納\n",
        "\n",
        "    #静止区間を除去\n",
        "    ResultantAcc = []\n",
        "    for i in range(len(Hz)):\n",
        "        ResultantAcc.append(acc_to_remove_all_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "      features[i][2] = np.var(ResultantAcc[i])   #静止区間を除去した後の合成加速度の分散値を2列目に格納\n",
        "      features[i][3] = len(ResultantAcc[i])   #静止区間を除去した後のデータの長さを3列目に格納\n",
        "\n",
        "    #ヒストグラム作成\n",
        "    for i in range(len(ResultantAcc)):\n",
        "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "    DifferenceAcc_hist = create_histogram3(DifferenceAcc_list)\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "      features[i][4] = kurtosis(DifferenceAcc_hist[i])  #標準化前のヒストグラムの尖度を4列目に格納\n",
        "      features[i][5] = skew(DifferenceAcc_hist[i])  #標準化前のヒストグラムの歪度を5列目に格納\n",
        "\n",
        "    #標準化\n",
        "    scaler = StandardScaler()\n",
        "    # Fit the scaler to x_train\n",
        "    scaler.fit(DifferenceAcc_hist)\n",
        "    # Use the scaler to transform x_train and x_test\n",
        "    DifferenceAcc_hist = scaler.transform(DifferenceAcc_hist)\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "      features[i][6] = kurtosis(DifferenceAcc_hist[i])  #標準化後のヒストグラムの尖度を6列目に格納\n",
        "      features[i][7] = skew(DifferenceAcc_hist[i])  #標準化後のヒストグラムの歪度を7列目に格納\n",
        "\n",
        "    return features, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "an60NfeH9OGh"
      },
      "outputs": [],
      "source": [
        "BINS = 1000\n",
        "MAX_VALUE = 40"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my data +-8G 100Hz max40000 2,3,4\n",
        "\n",
        "\n",
        "#labeled max4 2,3,4(long)\n",
        "#selfBACK +-8G 100Hz max4 1,2,3\n",
        "#robust max4 1,2,3 (normal)\n",
        "#pamap2 +-16G 100Hz max4 (long)4,5,6\n",
        "\n",
        "#KU-HAR max40 1,2,3(short)"
      ],
      "metadata": {
        "id": "kaYYfA8vntLn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labeled\n",
        "FeaturesLa, HzLa = path_to_features(\"/content/drive/MyDrive/labeled/\")\n",
        "La = np.column_stack((FeaturesLa[:, 4], FeaturesLa[:, 5], np.sqrt(FeaturesLa[:, 2])))"
      ],
      "metadata": {
        "id": "evGQ0dZJPNTu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2, 4, 6km/hで5分厳密に取った時\n",
        "Features2, Hz2 = path_to_features(\"/content/drive/MyDrive/walk2km_per_hour/\")\n",
        "Features4, Hz4 = path_to_features(\"/content/drive/MyDrive/walk4km_per_hour/\")\n",
        "Features6, Hz6 = path_to_features(\"/content/drive/MyDrive/jog6km_per_hour/\")\n",
        "two_per_hour = np.column_stack((Features2[:, 4], Features2[:, 5], np.sqrt(Features2[:, 2])))\n",
        "four_per_hour = np.column_stack((Features4[:, 4], Features4[:, 5], np.sqrt(Features4[:, 2])))\n",
        "six_per_hour = np.column_stack((Features6[:, 4], Features6[:, 5], np.sqrt(Features6[:, 2])))"
      ],
      "metadata": {
        "id": "IepEI1apxo6U"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#robust\n",
        "FeaturesRo, HzRo = path_to_features(\"/content/drive/MyDrive/robust/\")\n",
        "Ro = np.column_stack((FeaturesRo[:, 4], FeaturesRo[:, 5], np.sqrt(FeaturesRo[:, 2])))"
      ],
      "metadata": {
        "id": "N7WWRYHyRenD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selfBACK\n",
        "FeaturesSBF, HzSBF = path_to_features(\"/content/drive/MyDrive/selfBACK/walk_fast/\")\n",
        "FeaturesSBM, HzSBM = path_to_features(\"/content/drive/MyDrive/selfBACK/walk_mod/\")\n",
        "FeaturesSBS, HzSBS = path_to_features(\"/content/drive/MyDrive/selfBACK/walk_slow/\")\n",
        "SBF = np.column_stack((FeaturesSBF[:, 4], FeaturesSBF[:, 5], np.sqrt(FeaturesSBF[:, 2])))\n",
        "SBM = np.column_stack((FeaturesSBM[:, 4], FeaturesSBM[:, 5], np.sqrt(FeaturesSBM[:, 2])))\n",
        "SBS = np.column_stack((FeaturesSBS[:, 4], FeaturesSBS[:, 5], np.sqrt(FeaturesSBS[:, 2])))"
      ],
      "metadata": {
        "id": "RTvgSBy7UqJI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pamap2\n",
        "FeaturesPMP, HzPMP = path_to_features(\"/content/drive/MyDrive/PAMAP2/\")\n",
        "PMP = np.column_stack((FeaturesPMP[:, 4], FeaturesPMP[:, 5], np.sqrt(FeaturesPMP[:, 2])))"
      ],
      "metadata": {
        "id": "_kNfIa2zVRx8",
        "outputId": "9fc2f3e1-30f7-4306-8b4f-2d120d81eea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
            "  return n/db/n.sum(), bin_edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DifferenceAcc_list, DifferenceAcc_hist, Hz = path_to_histogram(\"/content/drive/MyDrive/my_walk_data(100Hz15minutes)/\")"
      ],
      "metadata": {
        "id": "k_L9k8hbzeck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2, 4km/hで10分厳密に取った時\n",
        "Features2_10, Hz2_10 = path_to_features(\"/content/drive/MyDrive/walk2km_per_hour_10minutes/\")\n",
        "Features4_10, Hz4_10 = path_to_features(\"/content/drive/MyDrive/walk4km_per_hour_10minutes/\")\n",
        "two_per_hour_10 = np.column_stack((Features2_10[:, 4], Features2_10[:, 5], np.sqrt(Features2_10[:, 2])))\n",
        "four_per_hour_10 = np.column_stack((Features4_10[:, 4], Features4_10[:, 5], np.sqrt(Features4_10[:, 2])))"
      ],
      "metadata": {
        "id": "-prSW8Zb51Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kuhar\n",
        "FeaturesKHW, HzKHW = path_to_features(\"/content/drive/MyDrive/kuhar(walk)/\")\n",
        "FeaturesKHJ, HzKHJ = path_to_features(\"/content/drive/MyDrive/kuhar(jump)/\")\n",
        "FeaturesKHR, HzKHR = path_to_features(\"/content/drive/MyDrive/kuhar(run)/\")\n",
        "KHW = np.column_stack((FeaturesKHW[:, 4], FeaturesKHW[:, 5], np.sqrt(FeaturesKHW[:, 2])))\n",
        "KHJ = np.column_stack((FeaturesKHJ[:, 4], FeaturesKHJ[:, 5], np.sqrt(FeaturesKHJ[:, 2])))\n",
        "KHR = np.column_stack((FeaturesKHR[:, 4], FeaturesKHR[:, 5], np.sqrt(FeaturesKHR[:, 2])))"
      ],
      "metadata": {
        "id": "QDgr-0n7nSpY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pamap2\n",
        "FeaturesPMP, HzPMP = path_to_features(\"/content/drive/MyDrive/PAMAP2/\")\n",
        "PMP = np.column_stack((FeaturesPMP[:, 4], FeaturesPMP[:, 5], np.sqrt(FeaturesPMP[:, 2])))"
      ],
      "metadata": {
        "id": "_cwbMR-epGfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5分のをまとめるやつ\n",
        "#two_per_hour, four_per_hour, six_per_hour, two_per_hour_10, four_per_hour_10, Hz2, Hz4, Hz6, Hz2_10, Hz4_10,\n",
        "#all = np.concatenate([SBF, SBM, SBS, KHW, KHJ, KHR])\n",
        "#allHz = np.concatenate([HzSBF, HzSBM, HzSBS, HzKHW, HzKHJ, HzKHR])\n",
        "#全部まとめるやつ\n",
        "#all = np.concatenate([two_per_hour, four_per_hour, six_per_hour, two_per_hour_10, four_per_hour_10, Ro, La])\n",
        "#allHz = np.concatenate([Hz2, Hz4, Hz6, Hz2_10, Hz4_10, HzRo, HzLa])\n",
        "\n",
        "all = np.concatenate([Ro, La, SBF, SBM, SBS])\n",
        "allHz = np.concatenate([HzRo, HzLa, HzSBF, HzSBM, HzSBS])\n",
        "SB = np.concatenate([SBF, SBM, SBS])\n",
        "HzSB = np.concatenate([HzSBF, HzSBM, HzSBS])"
      ],
      "metadata": {
        "id": "MJRQGHbck9z8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = two_per_hour\n",
        "X_test =\n",
        "y_train = Hz2\n",
        "y_test = HzRo"
      ],
      "metadata": {
        "id": "KSo2EeUUcrfS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "g4EHbkt0aTx_",
        "outputId": "1ad0f040-70d2-4a3e-fd33-e55c396aec2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(288,)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5分のをまとめるやつ\n",
        "all = np.concatenate([two_per_hour, four_per_hour])\n",
        "allHz = np.concatenate([Hz2, Hz4])"
      ],
      "metadata": {
        "id": "rCWJSyi06DrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10分のをまとめるやつ\n",
        "all10 = np.concatenate([two_per_hour_10, four_per_hour_10])\n",
        "allHz10 = np.concatenate([Hz2_10, Hz4_10])"
      ],
      "metadata": {
        "id": "OXNZ-7AdhcD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#全部まとめるやつ\n",
        "all = np.concatenate([two_per_hour, four_per_hour, six_per_hour, two_per_hour_10, four_per_hour_10])\n",
        "allHz = np.concatenate([Hz2, Hz4, Hz6, Hz2_10, Hz4_10])"
      ],
      "metadata": {
        "id": "kcrJSiMBk1K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2, 6km/hのをまとめるやつ\n",
        "all = np.concatenate([two_per_hour, six_per_hour, two_per_hour_10])\n",
        "allHz = np.concatenate([Hz2, Hz6, Hz2_10])"
      ],
      "metadata": {
        "id": "3vfTXpnqnI0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4km/hのをまとめるやつ\n",
        "all10 = np.concatenate([four_per_hour, four_per_hour_10])\n",
        "allHz10 = np.concatenate([Hz4, Hz4_10])"
      ],
      "metadata": {
        "id": "zU1QSWcQnJ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = SB\n",
        "X_test = La\n",
        "y_train = HzSB\n",
        "y_test = HzLa"
      ],
      "metadata": {
        "id": "yfbpAEqxj5wN"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polynomial_features= PolynomialFeatures(degree=3)\n",
        "X_train = polynomial_features.fit_transform(X_train)\n",
        "X_test = polynomial_features.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "JUboxc7loYao"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "p6Y4PQn5Zm-6"
      },
      "outputs": [],
      "source": [
        "stdsc = StandardScaler()\n",
        "# 注意\n",
        "# 訓練用のデータを標準化\n",
        "X_train = stdsc.fit_transform(X_train)\n",
        "# 訓練用データを基準にテストデータも標準化\n",
        "X_test = stdsc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "JhuowTOmZ3jX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(X_test), 20, 100), tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "qwgk3bV6aLC1",
        "outputId": "552c20b2-5aa8-4516-897c-7eee8c1c8106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.09027777777778"
            ]
          },
          "metadata": {},
          "execution_count": 326
        }
      ],
      "source": [
        "mean_absolute_error(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(La, HzLa, test_size = 0.2)"
      ],
      "metadata": {
        "id": "3bNnrCd0crhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "87c19345-557e-4213-a930-1a3d3fca06ae"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-313-8f0e2ede1db5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    44X_train, X_test, y_train, y_test = train_test_split(La, HzLa, test_size = 0.2)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SB = np.concatenate([SBF, SBM, SBS])\n",
        "HzSB = np.concatenate([HzSBF, HzSBM, HzSBS])"
      ],
      "metadata": {
        "id": "HcIIhTpMM0_5"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMmWcGNYOnY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = []"
      ],
      "metadata": {
        "id": "C6SdiOWU5N2g"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #線形回帰\n",
        " for i in range(3000):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(SB, HzSB, test_size = 0.2)\n",
        "    polynomial_features= PolynomialFeatures(degree=3)\n",
        "    X_train = polynomial_features.fit_transform(X_train)\n",
        "    X_test = polynomial_features.fit_transform(X_test)\n",
        "    stdsc = StandardScaler()\n",
        "    # 注意\n",
        "    # 訓練用のデータを標準化\n",
        "    X_train = stdsc.fit_transform(X_train)\n",
        "    # 訓練用データを基準にテストデータも標準化\n",
        "    X_test = stdsc.transform(X_test)\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = tf.cast(tf.clip_by_value(model.predict(X_test), 20, 100), tf.int32)\n",
        "    mae.append(mean_absolute_error(y_pred, y_test))\n",
        "min(mae)"
      ],
      "metadata": {
        "id": "ejkFdG8k63E7",
        "outputId": "7b7435ce-6b6f-4f76-b6bd-408cf8e585b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.910614525139664"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#多項式回帰\n",
        "for i in range(1000):\n",
        "    polynomial_features= PolynomialFeatures(degree=3)\n",
        "    x_poly = polynomial_features.fit_transform(all)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x_poly, allHz, test_size = 0.2)\n",
        "    stdsc = StandardScaler()\n",
        "    # 注意\n",
        "    # 訓練用のデータを標準化\n",
        "    X_train = stdsc.fit_transform(X_train)\n",
        "    # 訓練用データを基準にテストデータも標準化\n",
        "    X_test = stdsc.transform(X_test)\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = tf.cast(tf.clip_by_value(model.predict(X_test), 20, 100), tf.int32)\n",
        "    mae.append(mean_absolute_error(y_pred, y_test))\n",
        "min(mae)"
      ],
      "metadata": {
        "id": "TZ-Hbgh0ndLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae"
      ],
      "metadata": {
        "id": "zeDMDEQslrv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.coef_"
      ],
      "metadata": {
        "id": "n3wzL5fA7EkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d0Isrwx2ABh"
      },
      "outputs": [],
      "source": [
        "# クラスごとに色分けされたプロットを作成\n",
        "plt.figure(figsize=(10, 10))\n",
        "#plt.scatter(HzN, FeaturesN[:, 4], alpha=0.3, c=\"b\", label=\"普通に歩いた時の尖度\")\n",
        "#plt.scatter(HzS, FeaturesS[:, 4], alpha=0.3, c=\"r\", label=\"遅めに歩いた時の尖度\")\n",
        "#plt.scatter(HzN, FeaturesN[:, 5], alpha=0.3, c=\"b\", label=\"普通に歩いた時の歪度\")\n",
        "#plt.scatter(HzS, FeaturesS[:, 5], alpha=0.3, c=\"r\", label=\"遅めに歩いた時の歪度\")\n",
        "#plt.scatter(HzN, np.sqrt(FeaturesN[:, 2]), alpha=0.3, c=\"b\", label=\"普通に歩いた時の標準偏差\")\n",
        "#plt.scatter(HzS, np.sqrt(FeaturesS[:, 2]), alpha=0.3, c=\"r\", label=\"遅めに歩いた時の標準偏差\")\n",
        "#plt.scatter(HzN, sinN, alpha=0.3, c=\"b\", label=\"普通に歩いた時の標準偏差*尖度\")\n",
        "#plt.scatter(HzS, sinS, alpha=0.3, c=\"r\", label=\"遅めに歩いた時の標準偏差*尖度\")\n",
        "\n",
        "#plt.scatter(Hz2, Features2[:, 4], alpha=0.3, c=\"r\", label=\"2km/hで歩いた時の尖度\")\n",
        "#plt.scatter(Hz4, Features4[:, 4], alpha=0.3, c=\"b\", label=\"4km/hで歩いた時の尖度\")\n",
        "#plt.scatter(Hz6, Features6[:, 4], alpha=0.3, c=\"y\", label=\"6km/hで走った時の尖度\")\n",
        "\n",
        "#plt.scatter(Hz2_10, Features2_10[:, 4], alpha=0.3, c=\"y\", label=\"2km/hで10分歩いた時の尖度\")\n",
        "#plt.scatter(Hz4_10, Features4_10[:, 4], alpha=0.3, c=\"g\", label=\"4km/hで10分歩いた時の尖度\")\n",
        "\n",
        "#plt.scatter(Hz2, Features2[:, 5], alpha=0.3, c=\"r\", label=\"2km/hで歩いた時の歪度\")\n",
        "#plt.scatter(Hz4, Features4[:, 5], alpha=0.3, c=\"b\", label=\"4km/hで歩いた時の歪度\")\n",
        "#plt.scatter(Hz6, Features6[:, 5], alpha=0.3, c=\"y\", label=\"6km/hで走った時の歪度\")\n",
        "\n",
        "#plt.scatter(Hz2_10, Features2_10[:, 5], alpha=0.3, c=\"y\", label=\"2km/hで10分歩いた時の歪度\")\n",
        "#plt.scatter(Hz4_10, Features4_10[:, 5], alpha=0.3, c=\"g\", label=\"4km/hで10分歩いた時の歪度\")\n",
        "\n",
        "#plt.scatter(Hz2, np.sqrt(Features2[:, 2]), alpha=0.3, c=\"r\", label=\"2km/hで歩いた時の標準偏差\")\n",
        "#plt.scatter(Hz4, np.sqrt(Features4[:, 2]), alpha=0.3, c=\"b\", label=\"4km/hで歩いた時の標準偏差\")\n",
        "#plt.scatter(Hz6, np.sqrt(Features6[:, 2]), alpha=0.3, c=\"y\", label=\"6km/hで走った時の標準偏差\")\n",
        "\n",
        "#plt.scatter(Hz2_10, np.sqrt(Features2_10[:, 2]), alpha=0.3, c=\"y\", label=\"2km/hで10分歩いた時の標準偏差\")\n",
        "#plt.scatter(Hz4_10, np.sqrt(Features4_10[:, 2]), alpha=0.3, c=\"g\", label=\"4km/hで10分歩いた時の標準偏差\")\n",
        "\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('root_var')\n",
        "#plt.ylim(-0.5, 4.5)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2v5Zc9Z4JDg"
      },
      "outputs": [],
      "source": [
        "pathHascJog = \"/content/drive/MyDrive/hasc(jog)/\"\n",
        "filename = os.listdir(pathHascJog) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "#filename.remove(\".DS_Store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OCGrnAR4Npx"
      },
      "outputs": [],
      "source": [
        "#使う変数を宣言\n",
        "readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "AccX, AccY, AccZ = [], [], []\n",
        "ResultantAcc = []\n",
        "DifferenceAcc_list = []\n",
        "\n",
        "#各データセットからデータを読み込み二次元配列に格納\n",
        "for i in filename:\n",
        "    readAccX, readAccY, readAccZ = get_acceleration(pathHascJog+i)\n",
        "    AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "HzHascJog = np.ones(len(filename)) * 100\n",
        "\n",
        "#各加速度データをダウンサンプリング\n",
        "for i in range(9, 1, -1):\n",
        "    AccX, AccY, AccZ, HzHascJog = resampling_Acc(100, i * 10, AccX, AccY, AccZ, HzHascJog)\n",
        "\n",
        "#静止区間を除去\n",
        "for i in range(len(HzHascJog)):\n",
        "    ResultantAcc.append(acc_to_remove_all_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "#ヒストグラム作成\n",
        "for i in range(len(ResultantAcc)):\n",
        "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "DifferenceAcc_histHascJog = create_histogram(DifferenceAcc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xd55xIABo2D"
      },
      "outputs": [],
      "source": [
        "#グラフを表示する領域を，figオブジェクトとして作成。\n",
        "fig = plt.figure(figsize = (9,6))\n",
        "xl1 = \"acceleration (0.1mG)\"\n",
        "yl1 = \"frequency\"\n",
        "\n",
        "#グラフを描画するsubplot領域を作成。\n",
        "ax1 = fig.add_subplot(3, 1, 1)\n",
        "ax2 = fig.add_subplot(3, 1, 2)\n",
        "ax3 = fig.add_subplot(3, 1, 3)\n",
        "\n",
        "#各subplot領域にデータを渡す(範囲指定)\n",
        "ax1.hist(DifferenceAcc_list[0], bins=BINS,density=True, range=(0, 1), label=\"100 Hz\")\n",
        "ax2.hist(DifferenceAcc_list[260], bins=BINS,density=True, range=(0, 1), label=\"50 Hz\")\n",
        "ax3.hist(DifferenceAcc_list[416], bins=BINS,density=True, range=(0, 1), label=\"20 Hz\")\n",
        "\n",
        "#各subplot領域にデータを渡す\n",
        "#ax1.hist(DifferenceAcc_list[0], bins=BINS,density=True, label=\"100 Hz\")\n",
        "#ax2.hist(DifferenceAcc_list[260], bins=BINS,density=True, label=\"50 Hz\")\n",
        "#ax3.hist(DifferenceAcc_list[416], bins=BINS,density=True, label=\"20 Hz\")\n",
        "\n",
        "#各subplotにxラベルを追加\n",
        "ax1.set_xlabel(xl1)\n",
        "ax2.set_xlabel(xl1)\n",
        "ax3.set_xlabel(xl1)\n",
        "\n",
        "#各subplotにyラベルを追加\n",
        "ax1.set_ylabel(yl1)\n",
        "ax2.set_ylabel(yl1)\n",
        "ax3.set_ylabel(yl1)\n",
        "\n",
        "#各subplotのy軸の範囲を指定\n",
        "ax1.set_ylim(0, 15)\n",
        "ax2.set_ylim(0, 15)\n",
        "ax3.set_ylim(0, 15)\n",
        "\n",
        "# 凡例表示\n",
        "ax1.legend(loc = 'upper right')\n",
        "ax2.legend(loc = 'upper right')\n",
        "ax3.legend(loc = 'upper right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChupxXw6i9Ml"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(DifferenceAcc_histW, HzW, test_size = 0.2)\n",
        "  model = Transformer_Encoder_S()\n",
        "  model.fit(stdW, HzW, batch_size=32, epochs=50, shuffle=True, validation_split=0.2)\n",
        "  y_pred = tf.cast(tf.clip_by_value(model.predict(stdJ), 20, 100), tf.int32)\n",
        "  mae.append(mean_absolute_error(y_pred, HzJ))\n",
        "mae"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}