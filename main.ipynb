{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 20:51:17.676009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#ライブラリをインポート\n",
    "import os #OSに依存する様々な機能を利用するためのモジュール(ファイルやディレクトリ操作など)\n",
    "import re #正規表現を利用するためのモジュール\n",
    "import csv  #csvファイルを扱うためのモジュール\n",
    "import math #数学的計算のためのモジュール\n",
    "import matplotlib.pyplot as plt #グラフ描画のためのモジュール\n",
    "import numpy as np  #多次元配列計算のためのモジュール\n",
    "import pandas as pd #データフレームを扱うためのモジュール\n",
    "from scipy import signal  #信号処理のためのモジュール\n",
    "from scipy.stats import skew, kurtosis  #歪度と尖度を調べるためのモジュール\n",
    "from sklearn.model_selection import train_test_split  #データをトレーニング用とテスト用に分けるためのモジュール\n",
    "from sklearn import preprocessing #データを正規化するためのモジュール\n",
    "from sklearn.preprocessing import StandardScaler  #データを標準化するためのモジュール\n",
    "from sklearn.preprocessing import LabelEncoder  #カテゴリ変数を数値化するためのモジュール\n",
    "from sklearn.linear_model import LinearRegression #線型回帰\n",
    "from sklearn.svm import SVC #サポートベクターマシン\n",
    "from sklearn.ensemble import RandomForestClassifier #ランダムフォレスト\n",
    "from sklearn.neighbors import KNeighborsClassifier  #k-近傍法\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score #機械学習モデルの性能評価のためのモジュール\n",
    "import xgboost as xgb #XGBoost\n",
    "import lightgbm as lgb  #LightGBM\n",
    "import tensorflow as tf #TensorFlow(Googleが開発したオープンソースの機械学習フレームワーク)\n",
    "from tensorflow import keras  #TensorFlow用のニューラルネットワークライブラリAPI\n",
    "from tensorflow.keras import layers #ニューラルネットワークのレイヤーを定義するためのモジュール\n",
    "import torch  #PyTorch\n",
    "import torch.nn as nn #ニューラルネットワークのためのモジュール\n",
    "import torch.optim as optim #パラメータの最適化を行うためのモジュール\n",
    "from torch.utils.data import DataLoader, Dataset  #データをバッチ単位でロードするためのユーティリティクラスz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定数を定義\n",
    "BINS = 4000  #ヒストグラムのビンの数\n",
    "EPSILON = .00001  #スムージングパラメータ\n",
    "UPPER_LIMIT = 1.1 #静止区間の上限\n",
    "LOWER_LIMIT = 0.9 #静止区間の加減\n",
    "STATIONARY_INTERVALS = 5  #静止区間除去のサンプルの間隔(静止区間が何サンプル連続したら除去するか)\n",
    "TRAIN_SIZE = 0.8  #機械学習のトレーニングデータの割合\n",
    "N_ESTIMATORS = 100  #決定木の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ディレクトリ内のAMWS020のデータセットのファイル名と周波数を取得する関数\n",
    "def get_Hz_and_filename(path: str) -> list[int, str]:\n",
    "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
    "    Hz_and_filename=[]  #ファイル名と周波数を格納するリストを宣言\n",
    "\n",
    "    for file in filename:\n",
    "        Hz = re.search(r'\\d+', file)    #正規表現を用いてファイル名の中で一番最初に出てくる数字(周波数)を取得\n",
    "        if Hz:  #数字の入っていないファイル名があるとエラーを吐くので、このif文でチェックする\n",
    "            Hz_and_filename.append([int(Hz.group(0)), file])    #ファイル名と周波数を格納\n",
    "\n",
    "    return Hz_and_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル名と周波数を分けて出力する関数\n",
    "def divide_Hz_and_filename(Hz_and_filename: list[int, str]) -> tuple[list[int], list[str]]:\n",
    "    Hz = []\n",
    "    filename = []\n",
    "    for row in Hz_and_filename:\n",
    "      Hz.append(row[0])\n",
    "      filename.append(row[1])\n",
    "\n",
    "    return Hz, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速度データのCSVファイルから3軸加速度を取得する関数\n",
    "def get_acceleration(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            AccX.append(float(row[2]))\n",
    "            AccY.append(float(row[3]))\n",
    "            AccZ.append(float(row[4]))\n",
    "\n",
    "    return AccX, AccY, AccZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#静止区間を除去する関数\n",
    "def acc_to_remove_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
    "    #各軸の加速度の平均を求める\n",
    "    AvgAccX = sum(AccX) / len(AccX)\n",
    "    AvgAccY = sum(AccY) / len(AccY)\n",
    "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
    "\n",
    "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
    "\n",
    "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
    "\n",
    "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
    "    i = 0 #ループ変数\n",
    "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
    "    while i < len(ResultantAcc):\n",
    "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
    "            counter += 1    #範囲内ならカウントを増やす\n",
    "            if counter == STATIONARY_INTERVALS: #カウントがSTATIONARY_INTERVALSに達したらその区間を削除\n",
    "                del ResultantAcc[i+1-STATIONARY_INTERVALS:i+1]    #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
    "                counter = 0 #カウンターをリセット\n",
    "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
    "        else:\n",
    "            counter = 0 #カウンターをリセット\n",
    "        i += 1\n",
    "\n",
    "    return ResultantAcc  #静止区間を除去した後のリストを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#連続する2サンプルの差分を取る関数\n",
    "def calculate_differences_of_acceleration(ResultantAcc: list[float]) -> list[float]:\n",
    "    DifferenceAcc = [math.fabs(ResultantAcc[i + 1] * 100000 - ResultantAcc[i] * 100000) for i in range(len(ResultantAcc) - 1)]  #100000倍して誤差を取る\n",
    "    return DifferenceAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLダイバージェンス関数 #引数として与える2つの分布は非負の値の集合でなければならないことに注意\n",
    "def KL_divergence(a: list[float], b: list[float]) -> float:\n",
    "    min_value = min(min(a), min(b)) #a,bの最小値の小さい方\n",
    "    max_value = max(max(a), max(b)) #a,bの最大値の大きい方\n",
    "\n",
    "    #a,bのヒストグラムを作成し、同じ数のビンで区切る\n",
    "    a_hist, _ = np.histogram(a, bins=BINS, range=(min_value, max_value))\n",
    "    b_hist, _ = np.histogram(b, bins=BINS, range=(min_value, max_value))\n",
    "\n",
    "    #正規化する(確率分布に変換する、合計を1にする)ために全合計で割る\n",
    "    a_hist = (a_hist + EPSILON) / a_hist.sum()\n",
    "    b_hist = (b_hist + EPSILON) / b_hist.sum()\n",
    "\n",
    "    #KLダイバージェンスの値を返す\n",
    "    return np.sum([ai * np.log(ai / bi) for ai, bi in zip(a_hist, b_hist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSダイバージェンス関数 #引数として与える2つの分布は非負の値の集合でなければならないことに注意\n",
    "def JS_divergence(a: list[float], b: list[float]) -> float:\n",
    "    min_value = min(min(a), min(b)) #a,bの最小値の小さい方\n",
    "    max_value = max(max(a), max(b)) #a,bの最大値の大きい方\n",
    "\n",
    "    #a,bのヒストグラムを作成し、同じ数のビンで区切る\n",
    "    a_hist, _ = np.histogram(a, bins=BINS, range=(min_value, max_value))\n",
    "    b_hist, _ = np.histogram(b, bins=BINS, range=(min_value, max_value))\n",
    "\n",
    "    #正規化する(確率分布に変換する、合計を1にする)ために全合計で割る\n",
    "    a_hist = (a_hist + EPSILON) / a_hist.sum()\n",
    "    b_hist = (b_hist + EPSILON) / b_hist.sum()\n",
    "\n",
    "    #2つの分布の平均値を求める\n",
    "    mean_hist = (a_hist + b_hist) / 2.0\n",
    "\n",
    "    #平均とそれぞれの分布のKLダイバージェンスを算出\n",
    "    kl_a = np.sum([ai * np.log(ai / bi) for ai, bi in zip(a_hist, mean_hist)])\n",
    "    kl_b = np.sum([ai * np.log(ai / bi) for ai, bi in zip(b_hist, mean_hist)])\n",
    "\n",
    "    #JSダイバージェンスの値を返す\n",
    "    return (kl_a + kl_b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データフレームの各行の中で2番目に小さい値が格納されている場所を調べる関数(最小値は同じ確率分布同士の0.0)\n",
    "def get_index_and_columns_of_second_smallest(df: pd.DataFrame) -> list[str, str]:\n",
    "    index_and_columns_of_second_smallest = []  #データフレームの中で2番目に小さい値が格納されている場所のインデックス名とカラム名を格納する変数\n",
    "    for i in range(len(df)):\n",
    "        sorted_row = df.iloc[i].sort_values()   #.ilocでデータフレームの要素を行、列の番号の添字で指定する    #各行の要素を昇順に並び替える\n",
    "        second_smallest_columns = sorted_row.index[1] #各行の2番目に小さい値が格納されているカラム[1]の名前を取得\n",
    "        #second_smallest_label = df.columns.get_loc(second_smallest_index)\n",
    "        index_and_columns_of_second_smallest.append((df.index[i], second_smallest_columns))    #インデックスとカラムのラベル名の組を二次元配列に追加\n",
    "    return index_and_columns_of_second_smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KL・JSダイバージェンスの推定精度を算出する関数\n",
    "def calculate_accuracy(index_and_columns_of_second_smallest: list[str, str]) -> tuple[float, list[int]]:\n",
    "    counter = 0\n",
    "    error_index_list = []\n",
    "    for i in range(len(index_and_columns_of_second_smallest)):\n",
    "        #インデックスとカラムのラベル名が同じならばカウンターを1増やす\n",
    "        if index_and_columns_of_second_smallest[i][0] == index_and_columns_of_second_smallest[i][1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            error_index_list.append(i)\n",
    "            print(f\"間違ってるやつは{i}番目の{index_and_columns_of_second_smallest[i][0]}と{index_and_columns_of_second_smallest[i][1]}です\")\n",
    "\n",
    "    return (counter / len(index_and_columns_of_second_smallest)) * 100, error_index_list  #精度を100分率で返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が全加速度の差分データの最小値〜最大値）\n",
    "def create_histogram(DifferenceAcc_list: list[float]) -> np.histogram:\n",
    "    min_value = min(map(lambda x:max(x), DifferenceAcc_list))   #入力されたリストの中で最も小さい数\n",
    "    max_value = max(map(lambda x:max(x), DifferenceAcc_list))   #入力されたリストの中で最も大きい数\n",
    "\n",
    "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
    "\n",
    "    for i in range(len(DifferenceAcc_list)):\n",
    "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "    return DifferenceAcc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が各加速度の差分データの最小値〜最大値）\n",
    "def create_histogram2(DifferenceAcc_list: list[float]) -> np.histogram:\n",
    "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
    "    for i in range(len(DifferenceAcc_list)):\n",
    "        min_value = min(DifferenceAcc_list[i])\n",
    "        max_value = max(DifferenceAcc_list[i])\n",
    "        #DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "    return DifferenceAcc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速度データのCSVファイルからヒストグラムを作成する一連の流れを自動化した関数\n",
    "def read_Acc_to_histogram(path: str) -> tuple[np.histogram, np.array]:\n",
    "    Hz_and_filename = get_Hz_and_filename(path)\n",
    "    Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "    Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "    Hz = np.array(Hz)\n",
    "\n",
    "    #使う変数を宣言\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    ResultantAcc = []\n",
    "    DifferenceAcc_list = []\n",
    "\n",
    "    #各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "    for i in filename:\n",
    "        AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "        ResultantAcc = acc_to_remove_stationary_intervals(AccX, AccY, AccZ)\n",
    "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc))\n",
    "\n",
    "    DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)\n",
    "    return DifferenceAcc_hist, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各加速度データをダウンサンプリングする関数\n",
    "def resampling_Acc(originHz: int, newHz: int, AccX: list[float], AccY: list[float], AccZ: list[float], Hz: np.array) -> tuple[list[float], list[float], list[float], np.array]:\n",
    "    i = 0   #カウンター変数\n",
    "\n",
    "    while (Hz[i] == originHz):\n",
    "        originlen = len(AccX[i])    #元々のデータの長さ\n",
    "        sampling_factor = int(originlen * (newHz/originHz)) #ダウンサンプリングした後のデータの長さ\n",
    "        newAccX = signal.resample(AccX[i], sampling_factor)    #データをダウンサンプリング\n",
    "        newAccY = signal.resample(AccY[i], sampling_factor)    #データをダウンサンプリング\n",
    "        newAccZ = signal.resample(AccZ[i], sampling_factor)    #データをダウンサンプリング\n",
    "        AccX.append(newAccX)   #ダウンサンプリングデータを加速度データに追加\n",
    "        AccY.append(newAccY)   #ダウンサンプリングデータを加速度データに追加\n",
    "        AccZ.append(newAccZ)   #ダウンサンプリングデータを加速度データに追加\n",
    "        Hz = np.append(Hz, newHz)   #ダウンサンプリングレートを追加\n",
    "        i += 1\n",
    "\n",
    "    return AccX, AccY, AccZ, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLダイバージェンスとJSダイバージェンス算出の一連の流れを自動化した関数\n",
    "def KL_and_JS(path: str):\n",
    "    Hz_and_filename = get_Hz_and_filename(path)\n",
    "    Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "    Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "    Hz = [str(hz) + \"Hz\" for hz in Hz]  #周波数の値+\"Hz\"のリストを作りデータフレームのラベルに用いる\n",
    "\n",
    "    #使う変数を宣言\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    ResultantAcc = []\n",
    "    DifferenceAcc_list = []\n",
    "    resultKLD = [[0.0 for j in range(len(filename))] for i in range(len(filename))]  # resultKLDの要素を0.0で初期化\n",
    "    resultJSD = [[0.0 for j in range(len(filename))] for i in range(len(filename))]  # resultKLDの要素を0.0で初期化\n",
    "    error_index_list = []\n",
    "\n",
    "    #各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "    for i in filename:\n",
    "        AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "        ResultantAcc = acc_to_remove_stationary_intervals(AccX, AccY, AccZ)\n",
    "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc))\n",
    "\n",
    "    #KLダイバージェンスの値を格納\n",
    "    for i in range(len(filename)):\n",
    "        for j in range(len(filename)):\n",
    "            resultKLD[i][j] = KL_divergence(DifferenceAcc_list[i], DifferenceAcc_list[j])\n",
    "\n",
    "    #JSダイバージェンスの値を格納\n",
    "    for i in range(len(filename)):\n",
    "        for j in range(len(filename)):\n",
    "            resultJSD[i][j] = JS_divergence(DifferenceAcc_list[i], DifferenceAcc_list[j])\n",
    "\n",
    "    #結果を出力\n",
    "    df_KLD = pd.DataFrame(resultKLD, index=Hz, columns=Hz)\n",
    "    display(df_KLD)\n",
    "    accuracyKLD, error_index_list = calculate_accuracy(get_index_and_columns_of_second_smallest(df_KLD))\n",
    "    for i in range(len(error_index_list)):\n",
    "        print(filename[error_index_list[i]])\n",
    "    print(f\"KLダイバージェンスによる推定精度は{accuracyKLD}%です\")\n",
    "\n",
    "    df_JSD = pd.DataFrame(resultJSD, index=Hz, columns=Hz)\n",
    "    display(df_JSD)\n",
    "    accuracyJSD, error_index_list = calculate_accuracy(get_index_and_columns_of_second_smallest(df_JSD))\n",
    "    for i in range(len(error_index_list)):\n",
    "        print(filename[error_index_list[i]])\n",
    "    print(f\"JSダイバージェンスによる推定精度は{accuracyJSD}%です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ランダムフォレストによる機械学習モデル構築と性能評価までを自動化した関数\n",
    "def random_forest(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "\n",
    "    # 学習する\n",
    "    clf = RandomForestClassifier(n_estimators=N_ESTIMATORS, random_state=1234)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-近傍法による機械学習モデル構築と性能評価までを自動化した関数\n",
    "def k_neighbors(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(DifferenceAcc_hist)\n",
    "    newdata = scaler.transform(DifferenceAcc_hist)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(newdata, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "\n",
    "    # 学習する\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoostによる機械学習モデル構築と性能評価までを自動化した関数\n",
    "def xgboost(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    Hz = le.fit_transform(Hz)\n",
    "    # 学習する\n",
    "    x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "    clf = xgb.XGBClassifier(objective='multi:softmax')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"my_walk_data(100Hz15minutes)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一連のやつ\n",
    "Hz_and_filename = get_Hz_and_filename(path)\n",
    "Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "Hz = np.array(Hz)\n",
    "\n",
    "#使う変数を宣言\n",
    "readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
    "AccX, AccY, AccZ = [], [], []\n",
    "ResultantAcc = []\n",
    "DifferenceAcc_list = []\n",
    "\n",
    "#各データセットからデータを読み込み二次元配列に格納\n",
    "for i in filename:\n",
    "    readAccX, readAccY, readAccZ = get_acceleration(path+i)\n",
    "    AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
    "\n",
    "#各加速度データをダウンサンプリング\n",
    "for i in range(9, 1, -1):\n",
    "    AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
    "\n",
    "#静止区間を除去\n",
    "for i in range(len(Hz)):\n",
    "    ResultantAcc.append(acc_to_remove_stationary_intervals(AccX[i], AccY[i], AccZ[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒストグラム作成\n",
    "for i in range(len(ResultantAcc)):\n",
    "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
    "DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,  80,\n",
       "        70,  70,  70,  70,  70,  70,  70,  70,  70,  70,  70,  70,  70,\n",
       "        60,  60,  60,  60,  60,  60,  60,  60,  60,  60,  60,  60,  60,\n",
       "        50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
       "        40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,\n",
       "        30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  30,\n",
       "        20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 4000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DifferenceAcc_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to x_train\n",
    "scaler.fit(DifferenceAcc_hist)\n",
    "\n",
    "# Use the scaler to transform x_train and x_test\n",
    "DifferenceAcc_hist = scaler.transform(DifferenceAcc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer regression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "NUM_HEADS = 16\n",
    "KEY_DIM = 128\n",
    "BINS = 4000\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (BINS,)\n",
    "\n",
    "#形を定義(このモジュールは行列でないとダメっぽい)\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "outputs = layers.Input(shape=output_shape)\n",
    "x = layers.Reshape((10, 400))(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Encoder\n",
    "#Multi-Head-Attention層\n",
    "x_tmp = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(x, x)\n",
    "\n",
    "#Add & Norm層\n",
    "x = layers.add([x, x_tmp])\n",
    "x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "x_tmp = x\n",
    "\n",
    "#Feed Forward層\n",
    "x = layers.Dense(400 * 4, use_bias=True, activation=\"relu\")(x)\n",
    "x = layers.Dense(400, use_bias=True)(x)\n",
    "\n",
    "#Add & Norm層\n",
    "x = layers.add([x, x_tmp])\n",
    "x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Decoder\n",
    "y = layers.Reshape((10, 400))(inputs)\n",
    "#Multi-Head-Attention層\n",
    "y = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM,  dropout=DROPOUT)(y, y, use_causal_mask=True)\n",
    "\n",
    "#Add & Norm層\n",
    "y = layers.add([y, y])\n",
    "y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "\n",
    "#Multi-Head-Attention層(encoderのやつも合わせる)\n",
    "x = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM, dropout=DROPOUT)(y, x)\n",
    "\n",
    "#Add & Norm層\n",
    "x = layers.add([x, x])\n",
    "x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "#Feed Forward層\n",
    "x = layers.Dense(400 * 4, use_bias=True, activation=\"relu\")(x)\n",
    "x = layers.Dense(400, use_bias=True)(x)\n",
    "\n",
    "#Add & Norm層\n",
    "x = layers.add([x, x])\n",
    "x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "#予測値をint型に限定\n",
    "#y_pred = tf.cast(model.predict(x_test), tf.int32)\n",
    "#予測値の範囲を限定\n",
    "#y_pred = tf.clip_by_value(model.predict(x_test), 20, 100)\n",
    "\n",
    "#予測値の型と範囲を限定\n",
    "y_pred = tf.cast(tf.clip_by_value(model.predict(x_test), 20, 100), tf.int32)\n",
    "\n",
    "#平均二乗誤差\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "\n",
    "#二乗平均平方誤差\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "#平均絶対誤差\n",
    "mae = mean_absolute_error(y_pred, y_test)\n",
    "\n",
    "#決定係数\n",
    "r2 = r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#旧Transformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#transformer regression\n",
    "# Define the input shape\n",
    "input_shape = (BINS,)\n",
    "\n",
    "# Define the model\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "x = layers.Reshape((100, 40))(inputs)\n",
    "#y = layers.Reshape((100, 40))(inputs)\n",
    "x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "#y = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, y)\n",
    "#x = layers.Flatten()(y)\n",
    "x = layers.Flatten()(x)\n",
    "#x = layers.Dense(256, activation=\"relu\")(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    "    metrics=[keras.metrics.mean_squared_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 5s 90ms/step - loss: 3657.7778 - mean_squared_error: 3657.7778 - val_loss: 3075.7417 - val_mean_squared_error: 3075.7417\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1695.0493 - mean_squared_error: 1695.0493 - val_loss: 1886.4652 - val_mean_squared_error: 1886.4652\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 1859.8503 - mean_squared_error: 1859.8503 - val_loss: 1088.9558 - val_mean_squared_error: 1088.9558\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1250.3910 - mean_squared_error: 1250.3910 - val_loss: 1715.9835 - val_mean_squared_error: 1715.9835\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 1373.4673 - mean_squared_error: 1373.4673 - val_loss: 1056.1260 - val_mean_squared_error: 1056.1260\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 1008.4493 - mean_squared_error: 1008.4493 - val_loss: 1280.3605 - val_mean_squared_error: 1280.3605\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 1065.0244 - mean_squared_error: 1065.0244 - val_loss: 1141.8026 - val_mean_squared_error: 1141.8026\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 971.6389 - mean_squared_error: 971.6389 - val_loss: 1138.9885 - val_mean_squared_error: 1138.9885\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 909.1891 - mean_squared_error: 909.1891 - val_loss: 935.8595 - val_mean_squared_error: 935.8595\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 902.6076 - mean_squared_error: 902.6076 - val_loss: 993.5938 - val_mean_squared_error: 993.5938\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 815.3713 - mean_squared_error: 815.3713 - val_loss: 1039.9852 - val_mean_squared_error: 1039.9852\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 763.5684 - mean_squared_error: 763.5684 - val_loss: 967.4208 - val_mean_squared_error: 967.4208\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 626.7010 - mean_squared_error: 626.7010 - val_loss: 893.3694 - val_mean_squared_error: 893.3694\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 642.7575 - mean_squared_error: 642.7575 - val_loss: 805.8794 - val_mean_squared_error: 805.8794\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 500.9745 - mean_squared_error: 500.9745 - val_loss: 714.0391 - val_mean_squared_error: 714.0391\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 456.8028 - mean_squared_error: 456.8028 - val_loss: 558.6583 - val_mean_squared_error: 558.6583\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 478.2793 - mean_squared_error: 478.2793 - val_loss: 362.4713 - val_mean_squared_error: 362.4713\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 424.6113 - mean_squared_error: 424.6113 - val_loss: 305.8379 - val_mean_squared_error: 305.8379\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 372.2265 - mean_squared_error: 372.2265 - val_loss: 341.9867 - val_mean_squared_error: 341.9867\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 374.3390 - mean_squared_error: 374.3390 - val_loss: 390.2817 - val_mean_squared_error: 390.2817\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 335.9654 - mean_squared_error: 335.9654 - val_loss: 406.0043 - val_mean_squared_error: 406.0042\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 347.3801 - mean_squared_error: 347.3801 - val_loss: 353.2796 - val_mean_squared_error: 353.2796\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 373.6334 - mean_squared_error: 373.6334 - val_loss: 308.9289 - val_mean_squared_error: 308.9289\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 389.2848 - mean_squared_error: 389.2848 - val_loss: 345.2887 - val_mean_squared_error: 345.2887\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 316.5041 - mean_squared_error: 316.5041 - val_loss: 289.8142 - val_mean_squared_error: 289.8142\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 306.3713 - mean_squared_error: 306.3713 - val_loss: 178.0559 - val_mean_squared_error: 178.0559\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 163.4017 - mean_squared_error: 163.4017 - val_loss: 77.6733 - val_mean_squared_error: 77.6733\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 130.0172 - mean_squared_error: 130.0172 - val_loss: 86.7092 - val_mean_squared_error: 86.7092\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 122.7937 - mean_squared_error: 122.7937 - val_loss: 59.3352 - val_mean_squared_error: 59.3352\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 67.9181 - mean_squared_error: 67.9181 - val_loss: 33.4274 - val_mean_squared_error: 33.4274\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 73.6741 - mean_squared_error: 73.6741 - val_loss: 50.7778 - val_mean_squared_error: 50.7778\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 74.6817 - mean_squared_error: 74.6817 - val_loss: 60.7632 - val_mean_squared_error: 60.7632\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 103.8363 - mean_squared_error: 103.8363 - val_loss: 52.8036 - val_mean_squared_error: 52.8036\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 67.7069 - mean_squared_error: 67.7069 - val_loss: 62.6425 - val_mean_squared_error: 62.6425\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 70.0643 - mean_squared_error: 70.0643 - val_loss: 60.9417 - val_mean_squared_error: 60.9417\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 58.7090 - mean_squared_error: 58.7090 - val_loss: 69.6940 - val_mean_squared_error: 69.6940\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 54.0834 - mean_squared_error: 54.0834 - val_loss: 35.4191 - val_mean_squared_error: 35.4191\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 88.3805 - mean_squared_error: 88.3805 - val_loss: 39.0727 - val_mean_squared_error: 39.0727\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 87.3929 - mean_squared_error: 87.3929 - val_loss: 71.2317 - val_mean_squared_error: 71.2317\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 95.2382 - mean_squared_error: 95.2382 - val_loss: 56.3178 - val_mean_squared_error: 56.3178\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 78.2326 - mean_squared_error: 78.2326 - val_loss: 73.0267 - val_mean_squared_error: 73.0267\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 65.9031 - mean_squared_error: 65.9031 - val_loss: 41.5717 - val_mean_squared_error: 41.5717\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 48.9296 - mean_squared_error: 48.9296 - val_loss: 57.0954 - val_mean_squared_error: 57.0954\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 59.3991 - mean_squared_error: 59.3991 - val_loss: 58.6319 - val_mean_squared_error: 58.6319\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 66.9830 - mean_squared_error: 66.9830 - val_loss: 59.3279 - val_mean_squared_error: 59.3279\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 43.7764 - mean_squared_error: 43.7764 - val_loss: 48.3193 - val_mean_squared_error: 48.3193\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 45.3724 - mean_squared_error: 45.3724 - val_loss: 68.8821 - val_mean_squared_error: 68.8821\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 81.6346 - mean_squared_error: 81.6346 - val_loss: 50.4285 - val_mean_squared_error: 50.4285\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 64.3176 - mean_squared_error: 64.3176 - val_loss: 49.5745 - val_mean_squared_error: 49.5745\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 58.9456 - mean_squared_error: 58.9456 - val_loss: 38.7051 - val_mean_squared_error: 38.7051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x197739950>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=16, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 176ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.916666666666667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tf.cast(tf.clip_by_value(model.predict(x_test), 20, 100), tf.int32)\n",
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一連のやつ\n",
    "Hz_and_filename = get_Hz_and_filename(path)\n",
    "Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "Hz = np.array(Hz)\n",
    "\n",
    "#使う変数を宣言\n",
    "AccX, AccY, AccZ = [], [], []\n",
    "ResultantAcc = []\n",
    "DifferenceAcc_list = []\n",
    "\n",
    "#各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "for i in filename:\n",
    "    AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "    ResultantAcc.append(acc_to_remove_stationary_intervals(AccX, AccY, AccZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ResultantAcc)):\n",
    "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
    "DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#尖度\n",
    "DifferenceAcc_kurtosis = np.zeros(len(DifferenceAcc_hist))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    DifferenceAcc_kurtosis[i] = kurtosis(DifferenceAcc_hist[i])\n",
    "\n",
    "#歪度\n",
    "DifferenceAcc_skewness = np.zeros(len(DifferenceAcc_hist))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    DifferenceAcc_skewness[i] = skew(DifferenceAcc_hist[i])\n",
    "\n",
    "#分散\n",
    "histogram_var = np.zeros(len(DifferenceAcc_list))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    histogram_var[i] = np.var(DifferenceAcc_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_newに尖度＋歪度+分散を連結したもの\n",
    "X_new = np.concatenate((DifferenceAcc_kurtosis.reshape(-1, 1), DifferenceAcc_skewness.reshape(-1, 1), histogram_var.reshape(-1, 1)), axis=1)\n",
    "#XはDifferenceAcc_hist+X_new\n",
    "X = np.concatenate((DifferenceAcc_hist, X_new), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "919e6181955fbc636a96e4fdb04fb1b969c9681582829f05a2534c8d07862e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
