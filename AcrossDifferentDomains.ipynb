{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oTsgcWLEa9Nt"
      },
      "outputs": [],
      "source": [
        "#ライブラリをインポート\n",
        "import os #OSに依存する様々な機能を利用するためのモジュール(ファイルやディレクトリ操作など)\n",
        "import re #正規表現を利用するためのモジュール\n",
        "import csv  #csvファイルを扱うためのモジュール\n",
        "import math #数学的計算のためのモジュール\n",
        "from decimal import Decimal #小数点桁落ちをなくすためのモジュール\n",
        "import matplotlib.pyplot as plt #グラフ描画のためのモジュール\n",
        "import numpy as np  #多次元配列計算のためのモジュール\n",
        "import pandas as pd #データフレームを扱うためのモジュール\n",
        "from scipy import signal  #信号処理のためのモジュール\n",
        "from scipy.stats import skew, kurtosis  #歪度と尖度を調べるためのモジュール\n",
        "from sklearn.model_selection import train_test_split  #データをトレーニング用とテスト用に分けるためのモジュール\n",
        "from sklearn import preprocessing #データを正規化するためのモジュール\n",
        "from sklearn.preprocessing import StandardScaler  #データを標準化するためのモジュール\n",
        "from sklearn.preprocessing import LabelEncoder  #カテゴリ変数を数値化するためのモジュール\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score #機械学習モデルの性能評価のためのモジュール\n",
        "import tensorflow as tf #TensorFlow(Googleが開発したオープンソースの機械学習フレームワーク)\n",
        "from tensorflow import keras  #TensorFlow用のニューラルネットワークライブラリAPI\n",
        "from tensorflow.keras import layers #ニューラルネットワークのレイヤーを定義するためのモジュール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrXcc8k3bDt9",
        "outputId": "7a15db51-bcf7-4801-ff63-b4da811baeab"
      },
      "outputs": [],
      "source": [
        "#Google colab用\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5OsmIbDSvG2",
        "outputId": "0f92b142-6117-4f94-dc61-0e3186e0b268"
      },
      "outputs": [],
      "source": [
        "#GPUを使うためのコマンド\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuJ52pHW_ajE"
      },
      "outputs": [],
      "source": [
        "#よく使うgit command\n",
        "\n",
        "#google colabと揃える\n",
        "#git fetch origin main\n",
        "#git reset --hard origin/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-E1jV0_ybIY8"
      },
      "outputs": [],
      "source": [
        "#定数を定義\n",
        "BINS = 100  #ヒストグラムのビンの数\n",
        "EPSILON = .00001  #スムージングパラメータ\n",
        "UPPER_LIMIT = 1.1 #静止区間の上限\n",
        "LOWER_LIMIT = 0.9 #静止区間の加減\n",
        "STATIONARY_INTERVALS = 5  #静止区間除去のサンプルの間隔(静止区間が何サンプル連続したら除去するか)\n",
        "TRAIN_SIZE = 0.8  #機械学習のトレーニングデータの割合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MBcD5SbVbtzz"
      },
      "outputs": [],
      "source": [
        "#Hascの加速度データのCSVファイルから3軸加速度を取得する関数\n",
        "def get_acceleration(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    with open(filename) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            AccX.append(float(row[1]))\n",
        "            AccY.append(float(row[2]))\n",
        "            AccZ.append(float(row[3]))\n",
        "\n",
        "    return AccX, AccY, AccZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NxK06DTFWWfJ"
      },
      "outputs": [],
      "source": [
        "#3軸合成加速度を計算する関数\n",
        "def acc_to_resultant(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
        "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
        "    return ResultantAcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ryj7pQ-TbxHG"
      },
      "outputs": [],
      "source": [
        "#静止区間を除去する関数(STATIONARY_INTERVALS分のみ)\n",
        "def acc_to_remove_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
        "    #各軸の加速度の平均を求める\n",
        "    AvgAccX = sum(AccX) / len(AccX)\n",
        "    AvgAccY = sum(AccY) / len(AccY)\n",
        "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
        "\n",
        "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
        "\n",
        "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
        "\n",
        "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
        "    i = 0 #ループ変数\n",
        "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
        "    while i < len(ResultantAcc):\n",
        "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
        "            counter += 1    #範囲内ならカウントを増やす\n",
        "            if counter == STATIONARY_INTERVALS: #カウントがSTATIONARY_INTERVALSに達したらその区間を削除\n",
        "                del ResultantAcc[i+1-STATIONARY_INTERVALS:i+1]    #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
        "                counter = 0 #カウンターをリセット\n",
        "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
        "        else:\n",
        "            counter = 0 #カウンターをリセット\n",
        "        i += 1\n",
        "\n",
        "    return ResultantAcc  #静止区間を除去した後のリストを返す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8bWHh1zqSvG6"
      },
      "outputs": [],
      "source": [
        "#静止区間を除去する関数(STATIONARY_INTERVALS以上続いた静止区間全て)\n",
        "def acc_to_remove_all_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
        "    #各軸の加速度の平均を求める\n",
        "    AvgAccX = sum(AccX) / len(AccX)\n",
        "    AvgAccY = sum(AccY) / len(AccY)\n",
        "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
        "\n",
        "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
        "\n",
        "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
        "\n",
        "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
        "    i = 0 #ループ変数\n",
        "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
        "    while i < len(ResultantAcc):\n",
        "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
        "            counter += 1    #範囲内ならカウントを増やす\n",
        "        else:\n",
        "            if counter >= STATIONARY_INTERVALS:\n",
        "                del ResultantAcc[i+1-counter:i+1]   #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
        "                counter = 0 #カウンターをリセット\n",
        "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
        "            else :\n",
        "                counter = 0 #カウンターをリセット\n",
        "        i += 1\n",
        "\n",
        "    return ResultantAcc  #静止区間を除去した後のリストを返す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UVjjm5C493em"
      },
      "outputs": [],
      "source": [
        "#連続する2サンプルの差分を取る関数\n",
        "def calculate_differences_of_acceleration(ResultantAcc: list[float]) -> list[float]:\n",
        "    DifferenceAcc = [math.fabs(Decimal(ResultantAcc[i + 1]) - Decimal(ResultantAcc[i])) for i in range(len(ResultantAcc) - 1)]  #連続する2サンプルの誤差を取る\n",
        "    return DifferenceAcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hbUSs34Ib4Z_"
      },
      "outputs": [],
      "source": [
        "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が各加速度の差分データの最小値〜最大値）\n",
        "def create_histogram(DifferenceAcc_list: list[float]) -> np.histogram:\n",
        "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
        "    for i in range(len(DifferenceAcc_list)):\n",
        "        min_value = min(DifferenceAcc_list[i])\n",
        "        max_value = max(DifferenceAcc_list[i])\n",
        "        #DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
        "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, density=True) #ヒストグラムを作成し、同じ数のビンで区切る\n",
        "    return DifferenceAcc_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DqeTnb2lb9wM"
      },
      "outputs": [],
      "source": [
        "#各加速度データをダウンサンプリングする関数\n",
        "def resampling_Acc(originHz: int, newHz: int, AccX: list[float], AccY: list[float], AccZ: list[float], Hz: np.array) -> tuple[list[float], list[float], list[float], np.array]:\n",
        "    i = 0   #カウンター変数\n",
        "\n",
        "    while (Hz[i] == originHz):\n",
        "        originlen = len(AccX[i])    #元々のデータの長さ\n",
        "        sampling_factor = int(originlen * (newHz/originHz)) #ダウンサンプリングした後のデータの長さ\n",
        "        newAccX = signal.resample(AccX[i], sampling_factor)    #データをダウンサンプリング\n",
        "        newAccY = signal.resample(AccY[i], sampling_factor)    #データをダウンサンプリング\n",
        "        newAccZ = signal.resample(AccZ[i], sampling_factor)    #データをダウンサンプリング\n",
        "        AccX.append(newAccX)   #ダウンサンプリングデータを加速度データに追加\n",
        "        AccY.append(newAccY)   #ダウンサンプリングデータを加速度データに追加\n",
        "        AccZ.append(newAccZ)   #ダウンサンプリングデータを加速度データに追加\n",
        "        Hz = np.append(Hz, newHz)   #ダウンサンプリングレートを追加\n",
        "        i += 1\n",
        "\n",
        "    return AccX, AccY, AccZ, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gp5b0edH5qfd"
      },
      "outputs": [],
      "source": [
        "#path名から標準化されたヒストグラムと正解ラベルを返す関数\n",
        "def path_to_histogram(path: str) -> tuple[np.array, np.array]:\n",
        "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "    #使う変数を宣言\n",
        "    readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    ResultantAcc = []\n",
        "    DifferenceAcc_list = []\n",
        "\n",
        "    #各データセットからデータを読み込み二次元配列に格納\n",
        "    for i in filename:\n",
        "        readAccX, readAccY, readAccZ = get_acceleration(path+i)\n",
        "        AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "    Hz = np.ones(len(filename)) * 100\n",
        "\n",
        "    #各加速度データをダウンサンプリング\n",
        "    for i in range(9, 1, -1):\n",
        "        AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
        "\n",
        "    #静止区間を除去\n",
        "    for i in range(len(Hz)):\n",
        "        ResultantAcc.append(acc_to_remove_all_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "    #ヒストグラム作成\n",
        "    for i in range(len(ResultantAcc)):\n",
        "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "    DifferenceAcc_hist = create_histogram(DifferenceAcc_list)\n",
        "\n",
        "    #標準化\n",
        "    scaler = StandardScaler()\n",
        "    # Fit the scaler to x_train\n",
        "    scaler.fit(DifferenceAcc_hist)\n",
        "    # Use the scaler to transform x_train and x_test\n",
        "    DifferenceAcc_hist = scaler.transform(DifferenceAcc_hist)\n",
        "\n",
        "    return DifferenceAcc_hist, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cT-Q5MevWWfN"
      },
      "outputs": [],
      "source": [
        "#path名から標準化された様々な特徴量と正解ラベルを返す関数\n",
        "def path_to_features(path: str) -> tuple[np.array, np.array]:\n",
        "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "    #使う変数を宣言\n",
        "    readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "    AccX, AccY, AccZ = [], [], []\n",
        "    ResultantAcc = []\n",
        "    DifferenceAcc_list = []\n",
        "\n",
        "    #各データセットからデータを読み込み二次元配列に格納\n",
        "    for i in filename:\n",
        "        readAccX, readAccY, readAccZ = get_acceleration(path+i)\n",
        "        AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "    Hz = np.ones(len(filename)) * 100\n",
        "\n",
        "    #各加速度データをダウンサンプリング\n",
        "    for i in range(9, 1, -1):\n",
        "        AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
        "\n",
        "    features = np.ones((len(Hz), 6))    #各特徴量を入れる変数でこの関数の返り値\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "        ResultantAcc.append(acc_to_resultant(AccX[i], AccY[i], AccZ[i]))\n",
        "        features[i][0] = np.var(ResultantAcc[i])   #静止区間を除去する前の合成加速度の分散値を0列目に格納\n",
        "\n",
        "    #静止区間を除去\n",
        "    ResultantAcc = []\n",
        "    for i in range(len(Hz)):\n",
        "        ResultantAcc.append(acc_to_remove_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "      features[i][1] = np.var(ResultantAcc[i])   #静止区間を除去した後の合成加速度の分散値を11列目に格納\n",
        "\n",
        "    #ヒストグラム作成\n",
        "    for i in range(len(ResultantAcc)):\n",
        "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "    DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)\n",
        "\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "      features[i][2] = kurtosis(DifferenceAcc_hist[i])\n",
        "      features[i][3] = skew(DifferenceAcc_hist[i])\n",
        "\n",
        "    #標準化\n",
        "    scaler = StandardScaler()\n",
        "    # Fit the scaler to x_train\n",
        "    scaler.fit(DifferenceAcc_hist)\n",
        "    # Use the scaler to transform x_train and x_test\n",
        "    DifferenceAcc_hist = scaler.transform(DifferenceAcc_hist)\n",
        "\n",
        "    for i in range(len(Hz)):\n",
        "      features[i][4] = kurtosis(DifferenceAcc_hist[i])\n",
        "      features[i][5] = skew(DifferenceAcc_hist[i])\n",
        "\n",
        "    #標準化\n",
        "    scaler = StandardScaler()\n",
        "    # Fit the scaler to x_train\n",
        "    scaler.fit(features)\n",
        "    # Use the scaler to transform x_train and x_test\n",
        "    features = scaler.transform(features)\n",
        "\n",
        "    return features, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIQXdUwvb-gQ"
      },
      "outputs": [],
      "source": [
        "pathHasc = \"/content/drive/MyDrive/hasc(walk)/\"\n",
        "filename = os.listdir(pathHasc) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "#filename.remove(\".DS_Store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "pathHasc = \"hasc(walk)/\"\n",
        "filename = os.listdir(pathHasc) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "#filename.remove(\".DS_Store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IgkM-EBSvG-"
      },
      "outputs": [],
      "source": [
        "#使う変数を宣言\n",
        "readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "AccX, AccY, AccZ = [], [], []\n",
        "ResultantAcc = []\n",
        "DifferenceAcc_list = []\n",
        "\n",
        "#各データセットからデータを読み込み二次元配列に格納\n",
        "for i in filename:\n",
        "    readAccX, readAccY, readAccZ = get_acceleration(pathHasc+i)\n",
        "    AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "HzHasc = np.ones(len(filename)) * 100\n",
        "\n",
        "#各加速度データをダウンサンプリング\n",
        "for i in range(9, 1, -1):\n",
        "    AccX, AccY, AccZ, HzHasc = resampling_Acc(100, i * 10, AccX, AccY, AccZ, HzHasc)\n",
        "\n",
        "#静止区間を除去\n",
        "for i in range(len(HzHasc)):\n",
        "    ResultantAcc.append(acc_to_remove_all_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "\n",
        "#ヒストグラム作成\n",
        "for i in range(len(ResultantAcc)):\n",
        "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "DifferenceAcc_histHasc = create_histogram(DifferenceAcc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "ON2HDNrBAiPw",
        "outputId": "37c2a81d-e243-4247-a73e-39653c338371"
      },
      "outputs": [],
      "source": [
        "#グラフを表示する領域を，figオブジェクトとして作成。\n",
        "fig = plt.figure(figsize = (9,6))\n",
        "xl1 = \"acceleration (0.1mG)\"\n",
        "yl1 = \"frequency\"\n",
        "\n",
        "#グラフを描画するsubplot領域を作成。\n",
        "ax1 = fig.add_subplot(3, 1, 1)\n",
        "ax2 = fig.add_subplot(3, 1, 2)\n",
        "ax3 = fig.add_subplot(3, 1, 3)\n",
        "\n",
        "#各subplot領域にデータを渡す(範囲指定)\n",
        "ax1.hist(DifferenceAcc_list[0], bins=BINS,density=True, range=(0, 1), label=\"100 Hz\")\n",
        "ax2.hist(DifferenceAcc_list[260], bins=BINS,density=True, range=(0, 1), label=\"50 Hz\")\n",
        "ax3.hist(DifferenceAcc_list[416], bins=BINS,density=True, range=(0, 1), label=\"20 Hz\")\n",
        "\n",
        "#各subplot領域にデータを渡す\n",
        "#ax1.hist(DifferenceAcc_list[0], bins=BINS,density=True, label=\"100 Hz\")\n",
        "#ax2.hist(DifferenceAcc_list[260], bins=BINS,density=True, label=\"50 Hz\")\n",
        "#ax3.hist(DifferenceAcc_list[416], bins=BINS,density=True, label=\"20 Hz\")\n",
        "\n",
        "#各subplotにxラベルを追加\n",
        "ax1.set_xlabel(xl1)\n",
        "ax2.set_xlabel(xl1)\n",
        "ax3.set_xlabel(xl1)\n",
        "\n",
        "#各subplotにyラベルを追加\n",
        "ax1.set_ylabel(yl1)\n",
        "ax2.set_ylabel(yl1)\n",
        "ax3.set_ylabel(yl1)\n",
        "\n",
        "#各subplotのy軸の範囲を指定\n",
        "ax1.set_ylim(0, 15)\n",
        "ax2.set_ylim(0, 15)\n",
        "ax3.set_ylim(0, 15)\n",
        "\n",
        "# 凡例表示\n",
        "ax1.legend(loc = 'upper right')\n",
        "ax2.legend(loc = 'upper right')\n",
        "ax3.legend(loc = 'upper right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2v5Zc9Z4JDg"
      },
      "outputs": [],
      "source": [
        "pathHascJog = \"/content/drive/MyDrive/hasc(jog)/\"\n",
        "filename = os.listdir(pathHascJog) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
        "#filename.remove(\".DS_Store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OCGrnAR4Npx"
      },
      "outputs": [],
      "source": [
        "#使う変数を宣言\n",
        "readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
        "AccX, AccY, AccZ = [], [], []\n",
        "ResultantAcc = []\n",
        "DifferenceAcc_list = []\n",
        "\n",
        "#各データセットからデータを読み込み二次元配列に格納\n",
        "for i in filename:\n",
        "    readAccX, readAccY, readAccZ = get_acceleration(pathHascJog+i)\n",
        "    AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
        "\n",
        "HzHascJog = np.ones(len(filename)) * 100\n",
        "\n",
        "#各加速度データをダウンサンプリング\n",
        "for i in range(9, 1, -1):\n",
        "    AccX, AccY, AccZ, HzHascJog = resampling_Acc(100, i * 10, AccX, AccY, AccZ, HzHascJog)\n",
        "\n",
        "#静止区間を除去\n",
        "for i in range(len(HzHascJog)):\n",
        "    ResultantAcc.append(acc_to_remove_all_stationary_intervals(AccX[i], AccY[i], AccZ[i]))\n",
        "\n",
        "#ヒストグラム作成\n",
        "for i in range(len(ResultantAcc)):\n",
        "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
        "DifferenceAcc_histHascJog = create_histogram(DifferenceAcc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "4Xd55xIABo2D",
        "outputId": "8fdbba87-332d-420d-cbb7-d784cf97ad2b"
      },
      "outputs": [],
      "source": [
        "#グラフを表示する領域を，figオブジェクトとして作成。\n",
        "fig = plt.figure(figsize = (9,6))\n",
        "xl1 = \"acceleration (0.1mG)\"\n",
        "yl1 = \"frequency\"\n",
        "\n",
        "#グラフを描画するsubplot領域を作成。\n",
        "ax1 = fig.add_subplot(3, 1, 1)\n",
        "ax2 = fig.add_subplot(3, 1, 2)\n",
        "ax3 = fig.add_subplot(3, 1, 3)\n",
        "\n",
        "#各subplot領域にデータを渡す(範囲指定)\n",
        "ax1.hist(DifferenceAcc_list[0], bins=BINS,density=True, range=(0, 1), label=\"100 Hz\")\n",
        "ax2.hist(DifferenceAcc_list[260], bins=BINS,density=True, range=(0, 1), label=\"50 Hz\")\n",
        "ax3.hist(DifferenceAcc_list[416], bins=BINS,density=True, range=(0, 1), label=\"20 Hz\")\n",
        "\n",
        "#各subplot領域にデータを渡す\n",
        "#ax1.hist(DifferenceAcc_list[0], bins=BINS,density=True, label=\"100 Hz\")\n",
        "#ax2.hist(DifferenceAcc_list[260], bins=BINS,density=True, label=\"50 Hz\")\n",
        "#ax3.hist(DifferenceAcc_list[416], bins=BINS,density=True, label=\"20 Hz\")\n",
        "\n",
        "#各subplotにxラベルを追加\n",
        "ax1.set_xlabel(xl1)\n",
        "ax2.set_xlabel(xl1)\n",
        "ax3.set_xlabel(xl1)\n",
        "\n",
        "#各subplotにyラベルを追加\n",
        "ax1.set_ylabel(yl1)\n",
        "ax2.set_ylabel(yl1)\n",
        "ax3.set_ylabel(yl1)\n",
        "\n",
        "#各subplotのy軸の範囲を指定\n",
        "ax1.set_ylim(0, 15)\n",
        "ax2.set_ylim(0, 15)\n",
        "ax3.set_ylim(0, 15)\n",
        "\n",
        "# 凡例表示\n",
        "ax1.legend(loc = 'upper right')\n",
        "ax2.legend(loc = 'upper right')\n",
        "ax3.legend(loc = 'upper right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWOu6J40VMnh"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(DifferenceAcc_histHasc,  HzHasc, test_size = 0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNtc5TQecof0"
      },
      "outputs": [],
      "source": [
        "#標準化\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler to x_train\n",
        "scaler.fit(DifferenceAcc_histHasc)\n",
        "# Use the scaler to transform x_train and x_test\n",
        "DifferenceAcc_histHasc = scaler.transform(DifferenceAcc_histHasc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcpn-euSXSqI"
      },
      "outputs": [],
      "source": [
        "mae=[]\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL1682QlbnW4",
        "outputId": "51441b6a-6e79-420c-e366-189c76dcba20"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.mean_squared_error,\n",
        "      metrics=[keras.metrics.mean_squared_error],\n",
        "  )\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=300, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(X_test), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, y_test))\n",
        "mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pn67fXrVmhO",
        "outputId": "fd7b4e01-6f1a-4e6e-9523-e0bae49c3ce2"
      },
      "outputs": [],
      "source": [
        "#HascWalkで構築→HascJogで試す\n",
        "for i in range(5):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(DifferenceAcc_histHasc, HzHasc, test_size = 0.2)\n",
        "  model.reset_states()\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.mean_squared_error,\n",
        "      metrics=[keras.metrics.mean_squared_error],\n",
        "  )\n",
        "  history = model.fit(X_train, y_train, batch_size=32, epochs=50, shuffle=True, validation_split=0.2)\n",
        "  y_pred = tf.cast(tf.clip_by_value(model.predict(X_test), 20, 100), tf.int32)\n",
        "  mae.append(mean_absolute_error(y_pred, y_test))\n",
        "\n",
        "mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8SRN1Jednsw"
      },
      "outputs": [],
      "source": [
        "#標準化\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler to x_train\n",
        "scaler.fit(DifferenceAcc_histHasc)\n",
        "# Use the scaler to transform x_train and x_test\n",
        "DifferenceAcc_histHasc = scaler.transform(DifferenceAcc_histHasc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_uUTxGG4tw7"
      },
      "outputs": [],
      "source": [
        "#標準化\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler to x_train\n",
        "scaler.fit(DifferenceAcc_histHascJog)\n",
        "# Use the scaler to transform x_train and x_test\n",
        "DifferenceAcc_histHascJog = scaler.transform(DifferenceAcc_histHascJog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egtt3ysZ6ROk"
      },
      "outputs": [],
      "source": [
        "HistHascWalk, HzHascWalk = path_to_histogram(\"/content/drive/MyDrive/hasc(walk)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHaBdOIRRrEE"
      },
      "outputs": [],
      "source": [
        "hw21, kw17, kr14, hj19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHGyXIbgRBx2"
      },
      "outputs": [],
      "source": [
        "FeaturesHascWalk, HzHascWalk = path_to_features(\"/content/drive/MyDrive/hasc(jog)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTxk62k2ZL8O"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(HHascWalk, HzHascWalk, test_size = 0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo9_B-yBZfxa"
      },
      "outputs": [],
      "source": [
        "# パラメータ（alpha）を探索するモジュール\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "# パラメータ（alpha）をプロットするモジュール\n",
        "from yellowbrick.regressor import AlphaSelection\n",
        "\n",
        "# Ridge 回帰を実行するモジュール（最小二乗法＋L2正則化項）\n",
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "cGnqMiCjZSs_",
        "outputId": "27a3ae78-6a56-4f9e-a210-b2084df9235a"
      },
      "outputs": [],
      "source": [
        "# パラメータ（alpha）の探索区間を設定\n",
        "alphas = np.logspace(-10, 1, 500)\n",
        "\n",
        "# 訓練データを交差検証し、最適な alpha を求める\n",
        "ridgeCV = RidgeCV(alphas = alphas)\n",
        "\n",
        "# alpha をプロットする\n",
        "visualizer = AlphaSelection(ridgeCV)\n",
        "visualizer.fit(X_train, y_train)\n",
        "\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRoPfrRvZmc0",
        "outputId": "aec67979-b6f9-4feb-8634-777667e117cd"
      },
      "outputs": [],
      "source": [
        "# Ridge回帰のインスタンスを作成\n",
        "ridge = Ridge(alpha = 0.029)\n",
        "\n",
        "# 訓練データからモデルを生成（最小二乗法＋正則化項）\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# 切片を出力\n",
        "print(ridge.intercept_)\n",
        "\n",
        "# 回帰係数（傾き）を出力\n",
        "print(ridge.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTlIdKGXazsG"
      },
      "outputs": [],
      "source": [
        "y_pred = tf.cast(tf.clip_by_value(ridge.predict(X_test), 20, 100), tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63JXe0nDbSDW",
        "outputId": "0f617208-e909-4239-d7d4-2cba41fa6ee1"
      },
      "outputs": [],
      "source": [
        "mean_absolute_error(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCh_1YL-bqKp"
      },
      "outputs": [],
      "source": [
        "# 訓練データに対するスコア\n",
        "print(ridge.score(X_train, y_train))\n",
        "\n",
        "# テストデータに対するスコア\n",
        "print(ridge.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j44t-_Eq8R3L"
      },
      "outputs": [],
      "source": [
        "HistHascJog, HzHascJog = path_to_histogram(\"/content/drive/MyDrive/hasc(jog)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eypgClhA8hL5"
      },
      "outputs": [],
      "source": [
        "HistHascSkip, HzHascSkip = path_to_histogram(\"/content/drive/MyDrive/hasc(skip)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAqSRcrR8t8a"
      },
      "outputs": [],
      "source": [
        "HistHascSequence, HzHascSequence = path_to_histogram(\"/content/drive/MyDrive/hasc(sequence)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Transformer_Encoder():\n",
        "      # Define the input shape\n",
        "      input_shape = (BINS,)\n",
        "      output_shape = (1,)\n",
        "\n",
        "      #形を定義(このモジュールは行列でないとダメっぽい)\n",
        "      inputs_encoder = layers.Input(shape=input_shape)\n",
        "      inputs_decoder = layers.Input(shape=output_shape)\n",
        "\n",
        "      #Encoderに対する入力の形状\n",
        "      x_encoder = layers.Reshape((1, 100))(inputs_encoder)\n",
        "\n",
        "      #Transformer Encoder Layer(BERT)\n",
        "      for i in range(N):\n",
        "            #Multi-Head-Attention Layer\n",
        "            attention_encoder = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM, use_bias=True)(x_encoder, x_encoder, x_encoder)\n",
        "\n",
        "            #Dropout Layer\n",
        "            attention_encoder = layers.Dropout(rate=DROPOUT)(attention_encoder)\n",
        "            #Add & Norm Layer\n",
        "            attention_encoder = layers.LayerNormalization()(x_encoder + attention_encoder)\n",
        "\n",
        "            #Feed-Forward-Network\n",
        "            ffn_encoder = layers.Dense(BINS * 4, use_bias=True, activation=\"relu\")(attention_encoder)\n",
        "            ffn_encoder = layers.Dense(BINS, use_bias=True)(ffn_encoder)\n",
        "\n",
        "            #Dropout Layer\n",
        "            ffn_encoder = layers.Dropout(rate=DROPOUT)(ffn_encoder)\n",
        "            #Add & Norm Layer\n",
        "            x_encoder = layers.LayerNormalization()(attention_encoder + ffn_encoder)\n",
        "\n",
        "      x = layers.Flatten()(x_encoder)\n",
        "      x = layers.Dense(32, activation=\"relu\")(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      x = layers.Dense(16, activation=\"relu\")(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      outputs = layers.Dense(1, activation=\"relu\")(x)\n",
        "\n",
        "      model = keras.Model(inputs=inputs_encoder, outputs=outputs)\n",
        "\n",
        "      # Compile the model\n",
        "      model.compile(\n",
        "            optimizer=keras.optimizers.Adam(),\n",
        "            loss=keras.losses.mean_squared_error,\n",
        "            metrics=[keras.metrics.mean_squared_error],\n",
        "      )\n",
        "\n",
        "      return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdkRN2n9dxeT"
      },
      "outputs": [],
      "source": [
        "#transformer regression\n",
        "#各種パラメータ\n",
        "NUM_HEADS = 2\n",
        "KEY_DIM = 50\n",
        "BINS = 100\n",
        "DROPOUT = 0.1\n",
        "N = 1\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (BINS,)\n",
        "output_shape = (1,)\n",
        "\n",
        "#形を定義(このモジュールは行列でないとダメっぽい)\n",
        "inputs_encoder = layers.Input(shape=input_shape)\n",
        "inputs_decoder = layers.Input(shape=output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DfD41iqd2fh"
      },
      "outputs": [],
      "source": [
        "#Encoderに対する入力の形状\n",
        "x_encoder = layers.Reshape((1, 100))(inputs_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdfPrQ4Rd558"
      },
      "outputs": [],
      "source": [
        "#Transformer Encoder Layer(BERT)\n",
        "for i in range(N):\n",
        "      #Multi-Head-Attention Layer\n",
        "      attention_encoder = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM, use_bias=True)(x_encoder, x_encoder, x_encoder)\n",
        "\n",
        "      #Dropout Layer\n",
        "      attention_encoder = layers.Dropout(rate=DROPOUT)(attention_encoder)\n",
        "      #Add & Norm Layer\n",
        "      attention_encoder = layers.LayerNormalization()(x_encoder + attention_encoder)\n",
        "\n",
        "      #Feed-Forward-Network\n",
        "      ffn_encoder = layers.Dense(BINS * 4, use_bias=True, activation=\"relu\")(attention_encoder)\n",
        "      ffn_encoder = layers.Dense(BINS, use_bias=True)(ffn_encoder)\n",
        "\n",
        "      #Dropout Layer\n",
        "      ffn_encoder = layers.Dropout(rate=DROPOUT)(ffn_encoder)\n",
        "      #Add & Norm Layer\n",
        "      x_encoder = layers.LayerNormalization()(attention_encoder + ffn_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAQZaPQHd8VO"
      },
      "outputs": [],
      "source": [
        "x = layers.Flatten()(x_encoder)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"relu\")(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-knZZwMd-n8"
      },
      "outputs": [],
      "source": [
        "model = keras.Model(inputs=inputs_encoder, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-IwMOl3eBLI"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.mean_squared_error,\n",
        "    metrics=[keras.metrics.mean_squared_error],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUrWKQJQ_5Es"
      },
      "outputs": [],
      "source": [
        "HistHascWalk, HzHascWalk = path_to_histogram(\"/content/drive/MyDrive/hasc(walk)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bgZN86F_8W8"
      },
      "outputs": [],
      "source": [
        "HistHascJog, HzHascJog = path_to_histogram(\"/content/drive/MyDrive/hasc(jog)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdMXPd0GACjt"
      },
      "outputs": [],
      "source": [
        "HistKuharWalk, HzKuharWalk = path_to_histogram(\"/content/drive/MyDrive/kuhar(walk)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcrZjoLtAAYW"
      },
      "outputs": [],
      "source": [
        "HistKuharRun, HzKuharRun = path_to_histogram(\"/content/drive/MyDrive/kuhar(run)/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzbKsZVmeHnW",
        "outputId": "24ea664d-dce0-4843-ad87-ed323632c2bf"
      },
      "outputs": [],
      "source": [
        "#HascWalkで構築→HascJogで試す\n",
        "history = model.fit(HistHascWalk, HzHascWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascJog), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascJog))\n",
        "mse.append(mean_squared_error(y_pred, HzHascJog))\n",
        "r2.append(r2_score(y_pred, HzHascJog))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YByg-iUqAaF7",
        "outputId": "9fe2e1f8-32f2-4f4e-ffff-ee26fa775b7c"
      },
      "outputs": [],
      "source": [
        "#HascWalkで構築→KuharWalkで試す\n",
        "history = model.fit(HistHascWalk, HzHascWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistKuharWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzKuharWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzKuharWalk))\n",
        "r2.append(r2_score(y_pred, HzKuharWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRqn2VLxAi5v",
        "outputId": "63b4c91b-34ee-4f99-cf22-77ef65176a25"
      },
      "outputs": [],
      "source": [
        "#HascWalkで構築→KuharRunで試す\n",
        "history = model.fit(HistHascWalk, HzHascWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistKuharRun), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzKuharRun))\n",
        "mse.append(mean_squared_error(y_pred, HzKuharRun))\n",
        "r2.append(r2_score(y_pred, HzKuharRun))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibBwiJGOAqH8",
        "outputId": "139df95f-f116-4848-8941-9fade959d06a"
      },
      "outputs": [],
      "source": [
        "#HascJogで構築→HascWalkで試す\n",
        "history = model.fit(HistHascJog, HzHascJog, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzHascWalk))\n",
        "r2.append(r2_score(y_pred, HzHascWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_N60Tb2Aq2z",
        "outputId": "d66ab4a6-5246-4ea6-cd51-8bf3beb1cbfd"
      },
      "outputs": [],
      "source": [
        "#HascJogで構築→KuharWalkで試す\n",
        "history = model.fit(HistHascJog, HzHascJog, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistKuharWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzKuharWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzKuharWalk))\n",
        "r2.append(r2_score(y_pred, HzKuharWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yfWxEDNAxBl",
        "outputId": "7bd27912-c7ec-4277-82ba-fd968a95e9b7"
      },
      "outputs": [],
      "source": [
        "#HascJogで構築→KuharRunで試す\n",
        "history = model.fit(HistHascJog, HzHascJog, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistKuharRun), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzKuharRun))\n",
        "mse.append(mean_squared_error(y_pred, HzKuharRun))\n",
        "r2.append(r2_score(y_pred, HzKuharRun))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cc26e96A2g6",
        "outputId": "2e2439bc-f122-4128-fde8-bf3da71ba8ad"
      },
      "outputs": [],
      "source": [
        "#KuharWalkで構築→HascWalkで試す\n",
        "history = model.fit(HistKuharWalk, HzKuharWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzHascWalk))\n",
        "r2.append(r2_score(y_pred, HzHascWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKowjMmPA-Wd",
        "outputId": "ca7a37ac-e7f0-4cce-df99-895553a51c23"
      },
      "outputs": [],
      "source": [
        "#KuharWalkで構築→HascJogで試す\n",
        "history = model.fit(HistKuharWalk, HzKuharWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascJog), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascJog))\n",
        "mse.append(mean_squared_error(y_pred, HzHascJog))\n",
        "r2.append(r2_score(y_pred, HzHascJog))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA4YWRPWBF4m",
        "outputId": "aaddea36-e598-412a-a9f6-c5c86a18fb2e"
      },
      "outputs": [],
      "source": [
        "#KuharWalkで構築→KuharRunで試す\n",
        "history = model.fit(HistKuharWalk, HzKuharWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistKuharRun), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzKuharRun))\n",
        "mse.append(mean_squared_error(y_pred, HzKuharRun))\n",
        "r2.append(r2_score(y_pred, HzKuharRun))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LmPtJspBGPF",
        "outputId": "b5e10548-3f38-4a85-a960-f9b218fda04b"
      },
      "outputs": [],
      "source": [
        "#KuharRunで構築→HascWalkで試す\n",
        "history = model.fit(HistKuharRun, HzKuharRun, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzHascWalk))\n",
        "r2.append(r2_score(y_pred, HzHascWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz9ho85nBN9j",
        "outputId": "0f1acdae-fb12-4ad9-f81f-c17047ec84a2"
      },
      "outputs": [],
      "source": [
        "#KuharRunで構築→HascJogで試す\n",
        "history = model.fit(HistKuharRun, HzKuharRun, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascJog), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascJog))\n",
        "mse.append(mean_squared_error(y_pred, HzHascJog))\n",
        "r2.append(r2_score(y_pred, HzHascJog))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJmbBeU2BODh",
        "outputId": "8560211e-73d7-4d37-c32c-c9fbd4084d9b"
      },
      "outputs": [],
      "source": [
        "#KuharRunで構築→KuharWalkで試す\n",
        "history = model.fit(HistKuharRun, HzKuharRun, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistKuharWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzKuharWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzKuharWalk))\n",
        "r2.append(r2_score(y_pred, HzKuharWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suVTYP0q-cUl"
      },
      "outputs": [],
      "source": [
        "#HascWalkで構築→HascSkipで試す\n",
        "history = model.fit(HistHascWalk, HzHascWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascSkip), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascSkip))\n",
        "mse.append(mean_squared_error(y_pred, HzHascSkip))\n",
        "r2.append(r2_score(y_pred, HzHascSkip))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cawog7E-km-"
      },
      "outputs": [],
      "source": [
        "#HascWalkで構築→HascSequenceで試す\n",
        "history = model.fit(HistHascWalk, HzHascWalk, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascSequence), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascSequence))\n",
        "mse.append(mean_squared_error(y_pred, HzHascSequence))\n",
        "r2.append(r2_score(y_pred, HzHascSequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLbCSxrQHdyL"
      },
      "outputs": [],
      "source": [
        "#HascJogで構築→HascWalkで試す\n",
        "history = model.fit(HistHascJog, HzHascJog, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzHascWalk))\n",
        "r2.append(r2_score(y_pred, HzHascWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCvUXFCI_Pj4"
      },
      "outputs": [],
      "source": [
        "#HascJogで構築→HascSkipで試す\n",
        "history = model.fit(HistHascJog, HzHascJog, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascSkip), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascSkip))\n",
        "mse.append(mean_squared_error(y_pred, HzHascSkip))\n",
        "r2.append(r2_score(y_pred, HzHascSkip))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKHt_6Cb_XLa"
      },
      "outputs": [],
      "source": [
        "#HascJogで構築→HascSequenceで試す\n",
        "history = model.fit(HistHascJog, HzHascJog, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascSequence), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascSequence))\n",
        "mse.append(mean_squared_error(y_pred, HzHascSequence))\n",
        "r2.append(r2_score(y_pred, HzHascSequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cm29w48_oej"
      },
      "outputs": [],
      "source": [
        "#HascSkipで構築→HascWalkで試す\n",
        "history = model.fit(HistHascSkip, HzHascSkip, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzHascWalk))\n",
        "r2.append(r2_score(y_pred, HzHascWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfjZBIWS_2AO"
      },
      "outputs": [],
      "source": [
        "#HascSkipで構築→HascJogで試す\n",
        "history = model.fit(HistHascSkip, HzHascSkip, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascJog), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascJog))\n",
        "mse.append(mean_squared_error(y_pred, HzHascJog))\n",
        "r2.append(r2_score(y_pred, HzHascJog))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVXWyOsrACio"
      },
      "outputs": [],
      "source": [
        "#HascSkipで構築→HascSequenceで試す\n",
        "history = model.fit(HistHascSkip, HzHascSkip, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascSequence), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascSequence))\n",
        "mse.append(mean_squared_error(y_pred, HzHascSequence))\n",
        "r2.append(r2_score(y_pred, HzHascSequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daDgIq5aAai8"
      },
      "outputs": [],
      "source": [
        "#HascSequenceで構築→HascWalkで試す\n",
        "history = model.fit(HistHascSequence, HzHascSequence, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascWalk), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascWalk))\n",
        "mse.append(mean_squared_error(y_pred, HzHascWalk))\n",
        "r2.append(r2_score(y_pred, HzHascWalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8rM0ImaAmqc"
      },
      "outputs": [],
      "source": [
        "#HascSequenceで構築→HascJogで試す\n",
        "history = model.fit(HistHascSequence, HzHascSequence, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascJog), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascJog))\n",
        "mse.append(mean_squared_error(y_pred, HzHascJog))\n",
        "r2.append(r2_score(y_pred, HzHascJog))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3kO6ErhA4ut"
      },
      "outputs": [],
      "source": [
        "#HascSequenceで構築→HascSkipで試す\n",
        "history = model.fit(HistHascSequence, HzHascSequence, batch_size=32, epochs=EPOCHS, shuffle=True, validation_split=0.2)\n",
        "y_pred = tf.cast(tf.clip_by_value(model.predict(HistHascSkip), 20, 100), tf.int32)\n",
        "mae.append(mean_absolute_error(y_pred, HzHascSkip))\n",
        "mse.append(mean_squared_error(y_pred, HzHascSkip))\n",
        "r2.append(r2_score(y_pred, HzHascSkip))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
