{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリをインポート\n",
    "import os #OSに依存する様々な機能を利用するためのモジュール(ファイルやディレクトリ操作など)\n",
    "import re #正規表現を利用するためのモジュール\n",
    "import csv  #csvファイルを扱うためのモジュール\n",
    "import math #数学的計算のためのモジュール\n",
    "import matplotlib.pyplot as plt #グラフ描画のためのモジュール\n",
    "import numpy as np  #多次元配列計算のためのモジュール\n",
    "import pandas as pd #データフレームを扱うためのモジュール\n",
    "from scipy import signal  #信号処理のためのモジュール\n",
    "from scipy.stats import skew, kurtosis  #歪度と尖度を調べるためのモジュール\n",
    "from sklearn.model_selection import train_test_split  #データをトレーニング用とテスト用に分けるためのモジュール\n",
    "from sklearn import preprocessing #データを正規化するためのモジュール\n",
    "from sklearn.preprocessing import StandardScaler  #データを標準化するためのモジュール\n",
    "from sklearn.preprocessing import LabelEncoder  #カテゴリ変数を数値化するためのモジュール\n",
    "from sklearn.linear_model import LinearRegression #線型回帰\n",
    "from sklearn.svm import SVC #サポートベクターマシン\n",
    "from sklearn.ensemble import RandomForestClassifier #ランダムフォレスト\n",
    "from sklearn.neighbors import KNeighborsClassifier  #k-近傍法\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score #機械学習モデルの性能評価のためのモジュール\n",
    "import xgboost as xgb #XGBoost\n",
    "import lightgbm as lgb  #LightGBM\n",
    "import tensorflow as tf #TensorFlow(Googleが開発したオープンソースの機械学習フレームワーク)\n",
    "from tensorflow import keras  #TensorFlow用のニューラルネットワークライブラリAPI\n",
    "from tensorflow.keras import layers #ニューラルネットワークのレイヤーを定義するためのモジュール\n",
    "import torch  #PyTorch\n",
    "import torch.nn as nn #ニューラルネットワークのためのモジュール\n",
    "import torch.optim as optim #パラメータの最適化を行うためのモジュール\n",
    "from torch.utils.data import DataLoader, Dataset  #データをバッチ単位でロードするためのユーティリティクラスz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定数を定義\n",
    "BINS = 500  #ヒストグラムのビンの数\n",
    "EPSILON = .00001  #スムージングパラメータ\n",
    "UPPER_LIMIT = 1.1 #静止区間の上限\n",
    "LOWER_LIMIT = 0.9 #静止区間の加減\n",
    "STATIONARY_INTERVALS = 5  #静止区間除去のサンプルの間隔(静止区間が何サンプル連続したら除去するか)\n",
    "TRAIN_SIZE = 0.8  #機械学習のトレーニングデータの割合\n",
    "N_ESTIMATORS = 100  #決定木の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ディレクトリ内のAMWS020のデータセットのファイル名と周波数を取得する関数\n",
    "def get_Hz_and_filename(path: str) -> list[int, str]:\n",
    "    filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
    "    Hz_and_filename=[]  #ファイル名と周波数を格納するリストを宣言\n",
    "\n",
    "    for file in filename:\n",
    "        Hz = re.search(r'\\d+', file)    #正規表現を用いてファイル名の中で一番最初に出てくる数字(周波数)を取得\n",
    "        if Hz:  #数字の入っていないファイル名があるとエラーを吐くので、このif文でチェックする\n",
    "            Hz_and_filename.append([int(Hz.group(0)), file])    #ファイル名と周波数を格納\n",
    "\n",
    "    return Hz_and_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル名と周波数を分けて出力する関数\n",
    "def divide_Hz_and_filename(Hz_and_filename: list[int, str]) -> tuple[list[int], list[str]]:\n",
    "    Hz = []\n",
    "    filename = []\n",
    "    for row in Hz_and_filename:\n",
    "      Hz.append(row[0])\n",
    "      filename.append(row[1])\n",
    "\n",
    "    return Hz, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速度データのCSVファイルから3軸加速度を取得する関数\n",
    "def get_acceleration(filename: str) -> tuple[list[float], list[float], list[float]]:\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            AccX.append(float(row[1]))\n",
    "            AccY.append(float(row[2]))\n",
    "            AccZ.append(float(row[3]))\n",
    "\n",
    "    return AccX, AccY, AccZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#静止区間を除去する関数\n",
    "def acc_to_remove_stationary_intervals(AccX: list[float], AccY: list[float], AccZ: list[float]) -> list[float]:\n",
    "    #各軸の加速度の平均を求める\n",
    "    AvgAccX = sum(AccX) / len(AccX)\n",
    "    AvgAccY = sum(AccY) / len(AccY)\n",
    "    AvgAccZ = sum(AccZ) / len(AccZ)\n",
    "\n",
    "    AvgResultantAcc = math.sqrt(AvgAccX ** 2 + AvgAccY ** 2 + AvgAccZ ** 2) #重力加速度の推定値=合成加速度の平均を求める\n",
    "\n",
    "    ResultantAcc = [math.sqrt(x ** 2 + y ** 2 + z ** 2) for x, y, z in zip(AccX, AccY, AccZ)]   #各時刻の合成加速度を求める\n",
    "\n",
    "    #各時刻の合成加速度から静止区間(重力加速度の推定値に近い値が一定以上以上連続している区間)を除去する\n",
    "    i = 0 #ループ変数\n",
    "    counter = 0 #静止区間がSTATIONARY_INTERVALS分続いているかをカウントする変数\n",
    "    while i < len(ResultantAcc):\n",
    "        if AvgResultantAcc * LOWER_LIMIT < ResultantAcc[i] < AvgResultantAcc * UPPER_LIMIT:   #平均のLOWER_LIMIT倍~UPPER_LIMIT倍の範囲を調べる\n",
    "            counter += 1    #範囲内ならカウントを増やす\n",
    "            if counter == STATIONARY_INTERVALS: #カウントがSTATIONARY_INTERVALSに達したらその区間を削除\n",
    "                del ResultantAcc[i+1-STATIONARY_INTERVALS:i+1]    #スライスでは選択範囲の開始位置startと終了位置stopを[start:stop]のように書くとstart <= x < stopの範囲が選択される #start番目の値は含まれるがstop番目の値は含まれない\n",
    "                counter = 0 #カウンターをリセット\n",
    "                i -= STATIONARY_INTERVALS   #削除した分インデックスがズレるので補正する\n",
    "        else:\n",
    "            counter = 0 #カウンターをリセット\n",
    "        i += 1\n",
    "\n",
    "    return ResultantAcc  #静止区間を除去した後のリストを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#連続する2サンプルの差分を取る関数\n",
    "def calculate_differences_of_acceleration(ResultantAcc: list[float]) -> list[float]:\n",
    "    DifferenceAcc = [math.fabs(ResultantAcc[i + 1] * 100000 - ResultantAcc[i] * 100000) for i in range(len(ResultantAcc) - 1)]  #100000倍して誤差を取る\n",
    "    return DifferenceAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLダイバージェンス関数 #引数として与える2つの分布は非負の値の集合でなければならないことに注意\n",
    "def KL_divergence(a: list[float], b: list[float]) -> float:\n",
    "    min_value = min(min(a), min(b)) #a,bの最小値の小さい方\n",
    "    max_value = max(max(a), max(b)) #a,bの最大値の大きい方\n",
    "\n",
    "    #a,bのヒストグラムを作成し、同じ数のビンで区切る\n",
    "    a_hist, _ = np.histogram(a, bins=BINS, range=(min_value, max_value))\n",
    "    b_hist, _ = np.histogram(b, bins=BINS, range=(min_value, max_value))\n",
    "\n",
    "    #正規化する(確率分布に変換する、合計を1にする)ために全合計で割る\n",
    "    a_hist = (a_hist + EPSILON) / a_hist.sum()\n",
    "    b_hist = (b_hist + EPSILON) / b_hist.sum()\n",
    "\n",
    "    #KLダイバージェンスの値を返す\n",
    "    return np.sum([ai * np.log(ai / bi) for ai, bi in zip(a_hist, b_hist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSダイバージェンス関数 #引数として与える2つの分布は非負の値の集合でなければならないことに注意\n",
    "def JS_divergence(a: list[float], b: list[float]) -> float:\n",
    "    min_value = min(min(a), min(b)) #a,bの最小値の小さい方\n",
    "    max_value = max(max(a), max(b)) #a,bの最大値の大きい方\n",
    "\n",
    "    #a,bのヒストグラムを作成し、同じ数のビンで区切る\n",
    "    a_hist, _ = np.histogram(a, bins=BINS, range=(min_value, max_value))\n",
    "    b_hist, _ = np.histogram(b, bins=BINS, range=(min_value, max_value))\n",
    "\n",
    "    #正規化する(確率分布に変換する、合計を1にする)ために全合計で割る\n",
    "    a_hist = (a_hist + EPSILON) / a_hist.sum()\n",
    "    b_hist = (b_hist + EPSILON) / b_hist.sum()\n",
    "\n",
    "    #2つの分布の平均値を求める\n",
    "    mean_hist = (a_hist + b_hist) / 2.0\n",
    "\n",
    "    #平均とそれぞれの分布のKLダイバージェンスを算出\n",
    "    kl_a = np.sum([ai * np.log(ai / bi) for ai, bi in zip(a_hist, mean_hist)])\n",
    "    kl_b = np.sum([ai * np.log(ai / bi) for ai, bi in zip(b_hist, mean_hist)])\n",
    "\n",
    "    #JSダイバージェンスの値を返す\n",
    "    return (kl_a + kl_b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データフレームの各行の中で2番目に小さい値が格納されている場所を調べる関数(最小値は同じ確率分布同士の0.0)\n",
    "def get_index_and_columns_of_second_smallest(df: pd.DataFrame) -> list[str, str]:\n",
    "    index_and_columns_of_second_smallest = []  #データフレームの中で2番目に小さい値が格納されている場所のインデックス名とカラム名を格納する変数\n",
    "    for i in range(len(df)):\n",
    "        sorted_row = df.iloc[i].sort_values()   #.ilocでデータフレームの要素を行、列の番号の添字で指定する    #各行の要素を昇順に並び替える\n",
    "        second_smallest_columns = sorted_row.index[1] #各行の2番目に小さい値が格納されているカラム[1]の名前を取得\n",
    "        #second_smallest_label = df.columns.get_loc(second_smallest_index)\n",
    "        index_and_columns_of_second_smallest.append((df.index[i], second_smallest_columns))    #インデックスとカラムのラベル名の組を二次元配列に追加\n",
    "    return index_and_columns_of_second_smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KL・JSダイバージェンスの推定精度を算出する関数\n",
    "def calculate_accuracy(index_and_columns_of_second_smallest: list[str, str]) -> tuple[float, list[int]]:\n",
    "    counter = 0\n",
    "    error_index_list = []\n",
    "    for i in range(len(index_and_columns_of_second_smallest)):\n",
    "        #インデックスとカラムのラベル名が同じならばカウンターを1増やす\n",
    "        if index_and_columns_of_second_smallest[i][0] == index_and_columns_of_second_smallest[i][1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            error_index_list.append(i)\n",
    "            print(f\"間違ってるやつは{i}番目の{index_and_columns_of_second_smallest[i][0]}と{index_and_columns_of_second_smallest[i][1]}です\")\n",
    "\n",
    "    return (counter / len(index_and_columns_of_second_smallest)) * 100, error_index_list  #精度を100分率で返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が全加速度の差分データの最小値〜最大値）\n",
    "def create_histogram(DifferenceAcc_list: list[float]) -> np.histogram:\n",
    "    min_value = min(map(lambda x:max(x), DifferenceAcc_list))   #入力されたリストの中で最も小さい数\n",
    "    max_value = max(map(lambda x:max(x), DifferenceAcc_list))   #入力されたリストの中で最も大きい数\n",
    "\n",
    "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
    "\n",
    "    for i in range(len(DifferenceAcc_list)):\n",
    "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "    return DifferenceAcc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力された加速度の差分のリストからヒストグラムを作る関数（ビンの範囲が各加速度の差分データの最小値〜最大値）\n",
    "def create_histogram2(DifferenceAcc_list: list[float]) -> np.histogram:\n",
    "    DifferenceAcc_hist = np.zeros((len(DifferenceAcc_list), BINS), dtype=float)\n",
    "    for i in range(len(DifferenceAcc_list)):\n",
    "        min_value = min(DifferenceAcc_list[i])\n",
    "        max_value = max(DifferenceAcc_list[i])\n",
    "        #DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS, range=(min_value, max_value)) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "        DifferenceAcc_hist[i], _ = np.histogram(DifferenceAcc_list[i], bins=BINS) #ヒストグラムを作成し、同じ数のビンで区切る\n",
    "    return DifferenceAcc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速度データのCSVファイルからヒストグラムを作成する一連の流れを自動化した関数\n",
    "def read_Acc_to_histogram(path: str) -> tuple[np.histogram, np.array]:\n",
    "    Hz_and_filename = get_Hz_and_filename(path)\n",
    "    Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "    Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "    Hz = np.array(Hz)\n",
    "\n",
    "    #使う変数を宣言\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    ResultantAcc = []\n",
    "    DifferenceAcc_list = []\n",
    "\n",
    "    #各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "    for i in filename:\n",
    "        AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "        ResultantAcc = acc_to_remove_stationary_intervals(AccX, AccY, AccZ)\n",
    "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc))\n",
    "\n",
    "    DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)\n",
    "    return DifferenceAcc_hist, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各加速度データをダウンサンプリングする関数\n",
    "def resampling_Acc(originHz: int, newHz: int, AccX: list[float], AccY: list[float], AccZ: list[float], Hz: np.array) -> tuple[list[float], list[float], list[float], np.array]:\n",
    "    i = 0   #カウンター変数\n",
    "\n",
    "    while (Hz[i] == originHz):\n",
    "        originlen = len(AccX[i])    #元々のデータの長さ\n",
    "        sampling_factor = int(originlen * (newHz/originHz)) #ダウンサンプリングした後のデータの長さ\n",
    "        newAccX = signal.resample(AccX[i], sampling_factor)    #データをダウンサンプリング\n",
    "        newAccY = signal.resample(AccY[i], sampling_factor)    #データをダウンサンプリング\n",
    "        newAccZ = signal.resample(AccZ[i], sampling_factor)    #データをダウンサンプリング\n",
    "        AccX.append(newAccX)   #ダウンサンプリングデータを加速度データに追加\n",
    "        AccY.append(newAccY)   #ダウンサンプリングデータを加速度データに追加\n",
    "        AccZ.append(newAccZ)   #ダウンサンプリングデータを加速度データに追加\n",
    "        Hz = np.append(Hz, newHz)   #ダウンサンプリングレートを追加\n",
    "        i += 1\n",
    "\n",
    "    return AccX, AccY, AccZ, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLダイバージェンスとJSダイバージェンス算出の一連の流れを自動化した関数\n",
    "def KL_and_JS(path: str):\n",
    "    Hz_and_filename = get_Hz_and_filename(path)\n",
    "    Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "    Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "    Hz = [str(hz) + \"Hz\" for hz in Hz]  #周波数の値+\"Hz\"のリストを作りデータフレームのラベルに用いる\n",
    "\n",
    "    #使う変数を宣言\n",
    "    AccX, AccY, AccZ = [], [], []\n",
    "    ResultantAcc = []\n",
    "    DifferenceAcc_list = []\n",
    "    resultKLD = [[0.0 for j in range(len(filename))] for i in range(len(filename))]  # resultKLDの要素を0.0で初期化\n",
    "    resultJSD = [[0.0 for j in range(len(filename))] for i in range(len(filename))]  # resultKLDの要素を0.0で初期化\n",
    "    error_index_list = []\n",
    "\n",
    "    #各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "    for i in filename:\n",
    "        AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "        ResultantAcc = acc_to_remove_stationary_intervals(AccX, AccY, AccZ)\n",
    "        DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc))\n",
    "\n",
    "    #KLダイバージェンスの値を格納\n",
    "    for i in range(len(filename)):\n",
    "        for j in range(len(filename)):\n",
    "            resultKLD[i][j] = KL_divergence(DifferenceAcc_list[i], DifferenceAcc_list[j])\n",
    "\n",
    "    #JSダイバージェンスの値を格納\n",
    "    for i in range(len(filename)):\n",
    "        for j in range(len(filename)):\n",
    "            resultJSD[i][j] = JS_divergence(DifferenceAcc_list[i], DifferenceAcc_list[j])\n",
    "\n",
    "    #結果を出力\n",
    "    df_KLD = pd.DataFrame(resultKLD, index=Hz, columns=Hz)\n",
    "    display(df_KLD)\n",
    "    accuracyKLD, error_index_list = calculate_accuracy(get_index_and_columns_of_second_smallest(df_KLD))\n",
    "    for i in range(len(error_index_list)):\n",
    "        print(filename[error_index_list[i]])\n",
    "    print(f\"KLダイバージェンスによる推定精度は{accuracyKLD}%です\")\n",
    "\n",
    "    df_JSD = pd.DataFrame(resultJSD, index=Hz, columns=Hz)\n",
    "    display(df_JSD)\n",
    "    accuracyJSD, error_index_list = calculate_accuracy(get_index_and_columns_of_second_smallest(df_JSD))\n",
    "    for i in range(len(error_index_list)):\n",
    "        print(filename[error_index_list[i]])\n",
    "    print(f\"JSダイバージェンスによる推定精度は{accuracyJSD}%です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ランダムフォレストによる機械学習モデル構築と性能評価までを自動化した関数\n",
    "def random_forest(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "\n",
    "    # 学習する\n",
    "    clf = RandomForestClassifier(n_estimators=N_ESTIMATORS, random_state=1234)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-近傍法による機械学習モデル構築と性能評価までを自動化した関数\n",
    "def k_neighbors(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(DifferenceAcc_hist)\n",
    "    newdata = scaler.transform(DifferenceAcc_hist)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(newdata, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "\n",
    "    # 学習する\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoostによる機械学習モデル構築と性能評価までを自動化した関数\n",
    "def xgboost(path: str):\n",
    "    DifferenceAcc_hist, Hz = read_Acc_to_histogram(path)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    Hz = le.fit_transform(Hz)\n",
    "    # 学習する\n",
    "    x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)\n",
    "    clf = xgb.XGBClassifier(objective='multi:softmax')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"正解率 = \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hasc/\"\n",
    "filename = os.listdir(path) #引数のパスのディレクトリの中のファイル名一覧を取得\n",
    "filename.remove(\".DS_Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使う変数を宣言\n",
    "readAccX, readAccY, readAccZ = [], [], []   #データ読み込む用\n",
    "AccX, AccY, AccZ = [], [], []\n",
    "ResultantAcc = []\n",
    "DifferenceAcc_list = []\n",
    "\n",
    "#各データセットからデータを読み込み二次元配列に格納\n",
    "for i in filename:\n",
    "    readAccX, readAccY, readAccZ = get_acceleration(path+i)\n",
    "    AccX.append(readAccX), AccY.append(readAccY), AccZ.append(readAccZ)\n",
    "\n",
    "Hz = np.ones(len(filename)) * 100\n",
    "\n",
    "#各加速度データをダウンサンプリング\n",
    "for i in range(9, 1, -1):\n",
    "    AccX, AccY, AccZ, Hz = resampling_Acc(100, i * 10, AccX, AccY, AccZ, Hz)\n",
    "\n",
    "#静止区間を除去\n",
    "for i in range(len(Hz)):\n",
    "    ResultantAcc.append(acc_to_remove_stationary_intervals(AccX[i], AccY[i], AccZ[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒストグラム作成\n",
    "for i in range(len(ResultantAcc)):\n",
    "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
    "DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 500)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DifferenceAcc_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to x_train\n",
    "scaler.fit(DifferenceAcc_hist)\n",
    "\n",
    "# Use the scaler to transform x_train and x_test\n",
    "DifferenceAcc_hist = scaler.transform(DifferenceAcc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer regression\n",
    "#各種パラメータ\n",
    "NUM_HEADS = 4\n",
    "KEY_DIM = 32\n",
    "BINS = 500\n",
    "DROPOUT = 0.1\n",
    "N = 2\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (BINS,)\n",
    "output_shape = (1,)\n",
    "\n",
    "#形を定義(このモジュールは行列でないとダメっぽい)\n",
    "inputs_encoder = layers.Input(shape=input_shape)\n",
    "inputs_decoder = layers.Input(shape=output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Reshape((10, 50))(inputs_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = layers.Reshape((1, 1))(inputs_decoder)\n",
    "y =tf.tile(y, [1, 10, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Encoder\n",
    "for i in range(N):\n",
    "      #Multi-Head-Attention層\n",
    "      attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(x, x, x)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      attention = layers.Dropout(rate=DROPOUT)(attention)\n",
    "      #Add & Norm層\n",
    "      attention = layers.LayerNormalization()(x + attention)\n",
    "\n",
    "      #Feed Forward層\n",
    "      ffn = layers.Dense(50 * 4, use_bias=True, activation=\"relu\")(attention)\n",
    "      ffn = layers.Dense(50, use_bias=True)(ffn)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      ffn = layers.Dropout(rate=DROPOUT)(ffn)\n",
    "      #Add & Norm層\n",
    "      x = layers.LayerNormalization()(attention + ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Decoder\n",
    "for i in range(N):\n",
    "      #Multi-Head-Attention層\n",
    "      attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(y, y, y)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      attention = layers.Dropout(rate=DROPOUT)(attention)\n",
    "      #Add & Norm層\n",
    "      y = layers.LayerNormalization()(y + attention)\n",
    "\n",
    "      #Multi-Head-Attention層\n",
    "      attention = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)(x, x, y, use_causal_mask=True)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      attention = layers.Dropout(rate=DROPOUT)(attention)\n",
    "      #Add & Norm層\n",
    "      attention = layers.LayerNormalization()(y + attention)\n",
    "\n",
    "      #Feed Forward層\n",
    "      ffn = layers.Dense(50 * 4, use_bias=True, activation=\"relu\")(attention)\n",
    "      ffn = layers.Dense(50, use_bias=True)(ffn)\n",
    "\n",
    "      #ドロップアウト層\n",
    "      ffn = layers.Dropout(rate=DROPOUT)(ffn)\n",
    "      #Add & Norm層\n",
    "      x = layers.LayerNormalization()(attention + ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[inputs_encoder, inputs_decoder], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    "    metrics=[keras.metrics.mean_squared_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " reshape_18 (Reshape)           (None, 10, 50)       0           ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " multi_head_attention_30 (Multi  (None, 10, 50)      26034       ['reshape_18[0][0]',             \n",
      " HeadAttention)                                                   'reshape_18[0][0]',             \n",
      "                                                                  'reshape_18[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_64 (Dropout)           (None, 10, 50)       0           ['multi_head_attention_30[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_50 (TFOpL  (None, 10, 50)      0           ['reshape_18[0][0]',             \n",
      " ambda)                                                           'dropout_64[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_50 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_50[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 10, 200)      10200       ['layer_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " dense_62 (Dense)               (None, 10, 50)       10050       ['dense_61[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)           (None, 10, 50)       0           ['dense_62[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_51 (TFOpL  (None, 10, 50)      0           ['layer_normalization_50[0][0]', \n",
      " ambda)                                                           'dropout_65[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_51 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_51[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_31 (Multi  (None, 10, 50)      26034       ['layer_normalization_51[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_51[0][0]', \n",
      "                                                                  'layer_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)           (None, 10, 50)       0           ['multi_head_attention_31[0][0]']\n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_52 (TFOpL  (None, 10, 50)      0           ['layer_normalization_51[0][0]', \n",
      " ambda)                                                           'dropout_66[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_19 (Reshape)           (None, 1, 1)         0           ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_52 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_52[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.tile_9 (TFOpLambda)         (None, 10, 50)       0           ['reshape_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 10, 200)      10200       ['layer_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_32 (Multi  (None, 10, 50)      26034       ['tf.tile_9[0][0]',              \n",
      " HeadAttention)                                                   'tf.tile_9[0][0]',              \n",
      "                                                                  'tf.tile_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 10, 50)       10050       ['dense_63[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)           (None, 10, 50)       0           ['multi_head_attention_32[0][0]']\n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)           (None, 10, 50)       0           ['dense_64[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_54 (TFOpL  (None, 10, 50)      0           ['tf.tile_9[0][0]',              \n",
      " ambda)                                                           'dropout_68[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_53 (TFOpL  (None, 10, 50)      0           ['layer_normalization_52[0][0]', \n",
      " ambda)                                                           'dropout_67[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_54 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_54[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " layer_normalization_53 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_53[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_33 (Multi  (None, 10, 50)      26034       ['layer_normalization_53[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_53[0][0]', \n",
      "                                                                  'layer_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)           (None, 10, 50)       0           ['multi_head_attention_33[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_55 (TFOpL  (None, 10, 50)      0           ['layer_normalization_54[0][0]', \n",
      " ambda)                                                           'dropout_69[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_55 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_55[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 10, 200)      10200       ['layer_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_34 (Multi  (None, 10, 50)      26034       ['layer_normalization_54[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_54[0][0]', \n",
      "                                                                  'layer_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 10, 50)       10050       ['dense_65[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)           (None, 10, 50)       0           ['multi_head_attention_34[0][0]']\n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)           (None, 10, 50)       0           ['dense_66[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_57 (TFOpL  (None, 10, 50)      0           ['layer_normalization_54[0][0]', \n",
      " ambda)                                                           'dropout_71[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_56 (TFOpL  (None, 10, 50)      0           ['layer_normalization_55[0][0]', \n",
      " ambda)                                                           'dropout_70[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_57 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_57[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " layer_normalization_56 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_56[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_35 (Multi  (None, 10, 50)      26034       ['layer_normalization_56[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_56[0][0]', \n",
      "                                                                  'layer_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)           (None, 10, 50)       0           ['multi_head_attention_35[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_58 (TFOpL  (None, 10, 50)      0           ['layer_normalization_57[0][0]', \n",
      " ambda)                                                           'dropout_72[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_58 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_58[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 10, 200)      10200       ['layer_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 10, 50)       10050       ['dense_67[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)           (None, 10, 50)       0           ['dense_68[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_59 (TFOpL  (None, 10, 50)      0           ['layer_normalization_58[0][0]', \n",
      " ambda)                                                           'dropout_73[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_59 (LayerN  (None, 10, 50)      100         ['tf.__operators__.add_59[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 500)          0           ['layer_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 128)          64128       ['flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 128)          0           ['dense_69[0][0]']               \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 64)           8256        ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 64)           0           ['dense_70[0][0]']               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 1)            65          ['dropout_75[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,653\n",
      "Trainable params: 310,653\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(DifferenceAcc_hist, Hz, train_size = TRAIN_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 9s 73ms/step - loss: 2210.6162 - mean_squared_error: 2210.6162 - val_loss: 626.9210 - val_mean_squared_error: 626.9210\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 779.1443 - mean_squared_error: 779.1443 - val_loss: 542.4584 - val_mean_squared_error: 542.4584\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 511.9103 - mean_squared_error: 511.9103 - val_loss: 354.9316 - val_mean_squared_error: 354.9316\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 428.5854 - mean_squared_error: 428.5854 - val_loss: 271.5880 - val_mean_squared_error: 271.5880\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 334.2831 - mean_squared_error: 334.2831 - val_loss: 384.0676 - val_mean_squared_error: 384.0676\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 258.9412 - mean_squared_error: 258.9412 - val_loss: 327.7680 - val_mean_squared_error: 327.7680\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 208.3255 - mean_squared_error: 208.3255 - val_loss: 259.9340 - val_mean_squared_error: 259.9340\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 176.8155 - mean_squared_error: 176.8155 - val_loss: 285.3400 - val_mean_squared_error: 285.3400\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 161.1464 - mean_squared_error: 161.1464 - val_loss: 313.9842 - val_mean_squared_error: 313.9842\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 142.4751 - mean_squared_error: 142.4751 - val_loss: 295.1380 - val_mean_squared_error: 295.1380\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 134.7215 - mean_squared_error: 134.7215 - val_loss: 318.5833 - val_mean_squared_error: 318.5833\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 124.0168 - mean_squared_error: 124.0168 - val_loss: 287.0025 - val_mean_squared_error: 287.0025\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 95.5407 - mean_squared_error: 95.5407 - val_loss: 300.7281 - val_mean_squared_error: 300.7281\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 86.0896 - mean_squared_error: 86.0896 - val_loss: 291.6966 - val_mean_squared_error: 291.6966\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 94.1562 - mean_squared_error: 94.1562 - val_loss: 262.6497 - val_mean_squared_error: 262.6497\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 110.4265 - mean_squared_error: 110.4265 - val_loss: 234.4336 - val_mean_squared_error: 234.4336\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 115.8536 - mean_squared_error: 115.8536 - val_loss: 248.2559 - val_mean_squared_error: 248.2559\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 88.4471 - mean_squared_error: 88.4471 - val_loss: 320.5038 - val_mean_squared_error: 320.5038\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 70.9040 - mean_squared_error: 70.9040 - val_loss: 358.4084 - val_mean_squared_error: 358.4084\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 84.6912 - mean_squared_error: 84.6912 - val_loss: 291.6998 - val_mean_squared_error: 291.6998\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 89.9021 - mean_squared_error: 89.9021 - val_loss: 302.4194 - val_mean_squared_error: 302.4194\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 72.4717 - mean_squared_error: 72.4717 - val_loss: 272.2126 - val_mean_squared_error: 272.2126\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 82.0450 - mean_squared_error: 82.0450 - val_loss: 265.9297 - val_mean_squared_error: 265.9297\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 58.1000 - mean_squared_error: 58.1000 - val_loss: 294.3586 - val_mean_squared_error: 294.3586\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 58.0079 - mean_squared_error: 58.0079 - val_loss: 238.1304 - val_mean_squared_error: 238.1304\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 46.4564 - mean_squared_error: 46.4564 - val_loss: 238.4830 - val_mean_squared_error: 238.4830\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 56.8777 - mean_squared_error: 56.8777 - val_loss: 273.4911 - val_mean_squared_error: 273.4911\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 70.6771 - mean_squared_error: 70.6771 - val_loss: 234.0010 - val_mean_squared_error: 234.0010\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 55.8873 - mean_squared_error: 55.8873 - val_loss: 209.3921 - val_mean_squared_error: 209.3921\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 44.4552 - mean_squared_error: 44.4552 - val_loss: 273.2291 - val_mean_squared_error: 273.2291\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 42.6236 - mean_squared_error: 42.6236 - val_loss: 245.1625 - val_mean_squared_error: 245.1625\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 43.9048 - mean_squared_error: 43.9048 - val_loss: 222.3010 - val_mean_squared_error: 222.3010\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 46.8650 - mean_squared_error: 46.8650 - val_loss: 215.0873 - val_mean_squared_error: 215.0873\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 51.3926 - mean_squared_error: 51.3926 - val_loss: 210.9587 - val_mean_squared_error: 210.9587\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 41.8063 - mean_squared_error: 41.8063 - val_loss: 251.0023 - val_mean_squared_error: 251.0023\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 59.2176 - mean_squared_error: 59.2176 - val_loss: 228.1540 - val_mean_squared_error: 228.1540\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 1s 30ms/step - loss: 54.8187 - mean_squared_error: 54.8187 - val_loss: 223.2971 - val_mean_squared_error: 223.2971\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 59.8709 - mean_squared_error: 59.8709 - val_loss: 215.4214 - val_mean_squared_error: 215.4214\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 48.9675 - mean_squared_error: 48.9675 - val_loss: 220.2977 - val_mean_squared_error: 220.2977\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 44.8865 - mean_squared_error: 44.8865 - val_loss: 238.9155 - val_mean_squared_error: 238.9155\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 35.8341 - mean_squared_error: 35.8341 - val_loss: 221.1086 - val_mean_squared_error: 221.1086\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 43.5944 - mean_squared_error: 43.5944 - val_loss: 226.0489 - val_mean_squared_error: 226.0489\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 43.1899 - mean_squared_error: 43.1899 - val_loss: 254.7050 - val_mean_squared_error: 254.7050\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 63.7094 - mean_squared_error: 63.7094 - val_loss: 232.1227 - val_mean_squared_error: 232.1227\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 58.1010 - mean_squared_error: 58.1010 - val_loss: 228.8575 - val_mean_squared_error: 228.8575\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 38.9286 - mean_squared_error: 38.9286 - val_loss: 244.2218 - val_mean_squared_error: 244.2218\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 38.5153 - mean_squared_error: 38.5153 - val_loss: 237.1417 - val_mean_squared_error: 237.1417\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 37.8775 - mean_squared_error: 37.8775 - val_loss: 227.8523 - val_mean_squared_error: 227.8523\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 40.6766 - mean_squared_error: 40.6766 - val_loss: 218.0239 - val_mean_squared_error: 218.0239\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 42.6352 - mean_squared_error: 42.6352 - val_loss: 222.3713 - val_mean_squared_error: 222.3713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1482d48d0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train, y_train], y_train, batch_size=16, epochs=50, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.cast(tf.clip_by_value(model.predict([x_test, y_test]), 20, 100), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(78, 1), dtype=int32, numpy=\n",
       "array([[65],\n",
       "       [20],\n",
       "       [23],\n",
       "       [80],\n",
       "       [64],\n",
       "       [57],\n",
       "       [93],\n",
       "       [83],\n",
       "       [75],\n",
       "       [62],\n",
       "       [31],\n",
       "       [63],\n",
       "       [36],\n",
       "       [76],\n",
       "       [20],\n",
       "       [33],\n",
       "       [34],\n",
       "       [90],\n",
       "       [37],\n",
       "       [40],\n",
       "       [45],\n",
       "       [85],\n",
       "       [31],\n",
       "       [45],\n",
       "       [25],\n",
       "       [86],\n",
       "       [46],\n",
       "       [45],\n",
       "       [29],\n",
       "       [20],\n",
       "       [93],\n",
       "       [92],\n",
       "       [28],\n",
       "       [58],\n",
       "       [57],\n",
       "       [94],\n",
       "       [75],\n",
       "       [58],\n",
       "       [62],\n",
       "       [36],\n",
       "       [78],\n",
       "       [95],\n",
       "       [55],\n",
       "       [96],\n",
       "       [85],\n",
       "       [52],\n",
       "       [51],\n",
       "       [80],\n",
       "       [38],\n",
       "       [53],\n",
       "       [35],\n",
       "       [62],\n",
       "       [27],\n",
       "       [55],\n",
       "       [78],\n",
       "       [57],\n",
       "       [22],\n",
       "       [27],\n",
       "       [71],\n",
       "       [79],\n",
       "       [57],\n",
       "       [87],\n",
       "       [88],\n",
       "       [20],\n",
       "       [30],\n",
       "       [98],\n",
       "       [24],\n",
       "       [91],\n",
       "       [25],\n",
       "       [63],\n",
       "       [62],\n",
       "       [59],\n",
       "       [48],\n",
       "       [36],\n",
       "       [90],\n",
       "       [64],\n",
       "       [41],\n",
       "       [97]], dtype=int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  60,  60, 100,  50,  30,  80,  80,  60,  60,  80,  60,  50,\n",
       "        90,  80,  60,  40,  90,  60,  90,  50, 100,  90,  90,  30,  70,\n",
       "        70,  20, 100])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一連のやつ\n",
    "Hz_and_filename = get_Hz_and_filename(path)\n",
    "Hz_and_filename.sort(reverse=True)  #周波数の大きい順にソート\n",
    "Hz, filename = divide_Hz_and_filename(Hz_and_filename)\n",
    "Hz = np.array(Hz)\n",
    "\n",
    "#使う変数を宣言\n",
    "AccX, AccY, AccZ = [], [], []\n",
    "ResultantAcc = []\n",
    "DifferenceAcc_list = []\n",
    "\n",
    "#各データセットからデータを読み込み静止区間を除去したものを二次元配列に格納\n",
    "for i in filename:\n",
    "    AccX, AccY, AccZ = get_acceleration(path+i)\n",
    "    ResultantAcc.append(acc_to_remove_stationary_intervals(AccX, AccY, AccZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ResultantAcc)):\n",
    "    DifferenceAcc_list.append(calculate_differences_of_acceleration(ResultantAcc[i]))\n",
    "DifferenceAcc_hist = create_histogram2(DifferenceAcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#尖度\n",
    "DifferenceAcc_kurtosis = np.zeros(len(DifferenceAcc_hist))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    DifferenceAcc_kurtosis[i] = kurtosis(DifferenceAcc_hist[i])\n",
    "\n",
    "#歪度\n",
    "DifferenceAcc_skewness = np.zeros(len(DifferenceAcc_hist))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    DifferenceAcc_skewness[i] = skew(DifferenceAcc_hist[i])\n",
    "\n",
    "#分散\n",
    "histogram_var = np.zeros(len(DifferenceAcc_list))\n",
    "for i in range(len(DifferenceAcc_hist)):\n",
    "    histogram_var[i] = np.var(DifferenceAcc_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_newに尖度＋歪度+分散を連結したもの\n",
    "X_new = np.concatenate((DifferenceAcc_kurtosis.reshape(-1, 1), DifferenceAcc_skewness.reshape(-1, 1), histogram_var.reshape(-1, 1)), axis=1)\n",
    "#XはDifferenceAcc_hist+X_new\n",
    "X = np.concatenate((DifferenceAcc_hist, X_new), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "919e6181955fbc636a96e4fdb04fb1b969c9681582829f05a2534c8d07862e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
